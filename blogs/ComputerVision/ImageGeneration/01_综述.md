---
title: 图像生成综述
date: 2025/09/21
categories:
  - 计算机视觉
tags:
  - 图像生成
  - 综述
---

**图像生成（Image Generation）** 是计算机视觉与人工智能中的核心任务之一，旨在让机器==从数据分布中==学习视觉世界的规律，并能够生成符合这些规律的、具有逼真感与语义合理性的图像。它的核心要点有三层：

1. **分布学习**：图像生成并不是“凭空画图”，而是通过模型学习训练数据的统计分布。如今大多数的生成模型，从数学的角度上来看，都是概率与统计模型。模型在采样时，等价于“从学到的分布中抽取新的样本”。

2. **生成方式**：可以是 ==无条件生成==（仅依赖模型权重，随机噪声/隐变量等，采样得到图像）。也可以是 ==有条件生成==（在文本、标签、图像等条件引导下生成）。

3. **目标**：生成图像在 视觉质量（fidelity） 和 多样性（diversity） 上要与真实图像相当。同时逐步发展出 可控性（controllability） 和 跨模态性（cross-modality） 的能力，例如“文生图”、“图像编辑”。

对于不同的业务领域，图像生成还有很多的子任务划分，大致可以分类为以下几种：

## 1. 图像修复与复原

### 1.1 图像修复

==图像修复（Image Inpainting）== 并不是我们广义上理解的，所有缺陷都可以修复的。一般都是指 **基于 Mask 的内容补全**，不论这个图像是不是真的有残缺破损的内容缺失，我们都把需要修复的部分盖上一个 mask，当作这一部分缺失了，然后让模型进行推测和补全，这就是 Inpainting。

![1.1 Inpainting 案例 - Latent Diffusion Model](/cv/ImageGeneration/01_xxx/01_inpainting.png =560x)

我在制作我的数据集和读论文的过程中，发现还有一小类是归属于 Inpainting 范畴的，它是 ==色彩还原==。这个部分是说图像的纹理和结构都是完整的清晰的，但是出现了褪色、染色、发霉等情况，需要对色彩、亮度、对比度进行恢复或统一。

比如老照片的褪色修复，黑白图像上色（Colorization），色彩迁移这些。另外，去水印目前的技术一般都是基于 mask 的修复，但是我觉得有一类水印是可以归于色彩矫正的，**透明水印**。它只是文字或者 Logo 的周围有极小的阴影扰动，底层的纹理还是清晰可见的，但目前我并没有发现通过色彩校正的方式来消除透明水印的论文或方法。

![1.2 Colorization 案例 - DDColor Model](/cv/ImageGeneration/01_xxx/02_teaser.webp =560x)

### 1.2 图像复原

==图像复原/重建（Image Restoration）== 是一个更大范围的词，有的综述里把 Inpainting 也归于复原当中，但这里我们只说 **去噪、去模糊、超分辨率**。

关于==去噪（Denoising）==，我在《图像处理》中已经讲过噪声的来源，它的目的是移除这些噪声的同时，保持原始图像的纹理细节。传统方法多依赖滤波或稀疏表示，而深度学习方法则通过学习大规模数据分布，能够更好地在“去除噪声”与“保留细节”之间取得平衡。

![1.3 Denoising 案例 - noise2noise Model](/cv/ImageGeneration/01_xxx/03_denoise.png =560x)

**模糊图像**常来源于运动（如相机抖动、物体快速移动）或光学散焦，导致图像丢失高频细节。==去模糊（Deblurring）== 的目标是反卷积并恢复清晰的结构信息。早期方法基于物理模型估计模糊核，而现代深度模型则通过端到端学习完成去模糊，能够生成更自然、锐利的图像。

![1.4 Deblurring 案例 - DeblurDiNAT Model](/cv/ImageGeneration/01_xxx/04_deblurring.jpeg =560x)

==超分辨率（Super-Resolution, SR）== 旨在将低分辨率图像恢复为高分辨率图像。它不仅要求增加像素数量，还需要生成合理的细节与纹理。OpenCV 中的 Resize 是通过数学插值的方法来猜测出更多的像素，本质上没办法创造出丢失的细节，所以放大后的图像看起来模糊，缺乏高频细节。近年来基于深度学习的方法（如 SRCNN、ESRGAN）在自然图像、医学影像和卫星遥感中均取得了显著突破。

![1.5 Super-Resolution 案例 - Real-ESRGAN Model](/cv/ImageGeneration/01_xxx/05_sr.jpg =560x)

## 2. 图像风格与域转换

### 2.1 图像风格迁移

==图像风格迁移（Style Transfer）== 是指在保持原始图像内容和结构的前提下，将另一幅图像的艺术风格或视觉效果迁移过来。重点在于：**内容保持一致 → 风格发生改变。**

最早的代表是 神经风格迁移（Neural Style Transfer, 2016），通过 CNN 的特征分离，保留“内容特征”，替换“风格特征”。后续工作逐渐扩展到 快速风格迁移（Fast Style Transfer）、任意风格迁移（Arbitrary Style Transfer），甚至是 高分辨率 / 视频风格迁移。

近年来，Diffusion 与生成模型的发展，使得风格迁移不再局限于“梵高画风 → 照片”，而是可以在 色彩、纹理、光照、甚至领域（domain） 上进行灵活迁移，例如“照片转油画”、“卡通化”、“低光增强+风格化”。

![2.1 Style Transfer 案例 - visual-concept-translator Model](/cv/ImageGeneration/01_xxx/06_style_transfer.jpg =560x)

### 2.2 图像翻译

==图像翻译（I2I Translation）== 与风格迁移密切相关，但目标更广泛。它指的是**将一张输入图像转换为另一种表现形式，不仅仅是风格变化，还包括语义、域、模态的跨域转换**。典型例子包括：“素描 → 照片”、“标签图 → 场景”、“马 ↔ 斑马”、“夏天 ↔ 冬天”。

随着扩散模型和注意力机制的发展，近年 I2I 可以做到更高层次的**可控生成**。比如：条件引导（文本 + 图像），实现 文本驱动的图像翻译；保持输入图像结构，改变域或属性，例如“白天 → 夜晚”、“真实人像 → 动漫人物”。

![2.2 I2I Translation 案例 - CycleGAN Model](/cv/ImageGeneration/01_xxx/07_i2i_translation.jpg =560x)

风格迁移任务和翻译任务之间，对比下来可以这么理解：**风格迁移**是 I2I 的一个特殊子任务（内容不变，风格变）。**图像翻译**则涵盖了更多变化，比如 语义域改变、结构迁移、模态切换。

补充说明：I2I 是 Image-to-Image 的缩写，表示图像到图像任务，之前说的图像的修复与复原，包括现在的风格迁移与翻译，大多数的模型都是 I2I 任务，有少部分比较新的模型，会涉及多模态领域，会有 **文本到图像（Text-to-Image）** 与 **图像到文本（Image-to-text）** --> T2I、I2T 这样的缩写词。

## 3. 多模态 I2T

==图像到文本（I2T）== 是计算机视觉和自然语言处理的跨模态任务，旨在将视觉信息转化为自然语言描述。它不仅要求模型能够理解图像中的对象、场景和动作，还要能够生成语法正确、语义连贯的文本。

- 核心目标：将视觉信息 → 语言表示
- 输入：单张或多张图像
- 输出：单句描述、段落、故事或问答文本

#### 发展历程与关键技术：

1. **早期方法**：基于模板或检索的方法，生成静态描述，灵活性差。
2. **深度学习方法**：CNN + RNN 架构：提取图像特征 → 循环生成文本；注意力机制（Attention）：增强局部区域对描述的影响。
3. **Transformer**：Vision Transformer（ViT）+ Language Transformer
4. **多模态大模型的扩展**：BLIP、BLIP2、GIT 等统一图像和文本的生成与理解。支持图像到文本再到文本或文本到图像的交互；支持复杂任务，如视觉故事生成、图像问答。

图像到文本的任务，可以概括为 3 个大类：图像字幕、图像叙事、视觉问答。详细请看 3.1 到 3.3。

### 3.1 图像字幕

==图像字幕（Image Captioning）== 是计算机视觉和自然语言处理的交叉任务，旨在根据给定的图像生成自然语言描述。该任务不仅需要模型具备视觉理解能力，还需具备语言生成能力，以生成语法正确、语义丰富且与图像内容一致的文本。

该研究始于传统的计算机视觉方法，如基于模板的生成和图像检索。随着深度学习的发展，图像描述方法逐渐转向基于神经网络的模型，尤其是卷积神经网络（CNN）和循环神经网络（RNN）的结合。

在 Transformer 架构出现后，进一步推动了该领域的发展。到如今已经是多模态集成大模型的天下了，目前最火的应该就是 BLIP2、LAVIS 这些模型了。它还有一些更具体的分类：

1. **Image Captioning**：单张图像生成一句概括性描述，代表数据集为 MSCOCO, Flickr30k。

![3.1 Image Captioning 案例 - CLIP_prefix_caption Model](/cv/ImageGeneration/01_xxx/08_IC.jpeg =560x)

2. **Dense Captioning**：对图像多个区域生成局部描述，代表数据集为 Visual Genome。

![3.2 Dense Captioning 案例 - Densecap Model](/cv/ImageGeneration/01_xxx/08_DC.png =560x)

3. **Image Paragraph Generation**：复杂图像生成多句段落，强调上下文与细节，代表数据集为 Stanford Image Paragraph。

![3.3 Image Paragraph Generation 案例 - Image2Paragraph Model](/cv/ImageGeneration/01_xxx/08_IPG.png =560x)

### 3.2 图像叙事

==图像/视觉叙事（Image/Visual StoryTelling）== 是指给定一组（或一系列）图像，生成一段连贯的故事性文本。与传统的图像描述（caption）不同，图像叙事强调：

- 多张图像之间的逻辑关系、情节发展与连贯性；
- 文本不只是客观描述，还可能包含背景、推理、情感、过渡等叙事因素；
- 输出通常是多个句子、段落或故事，而非一句简单描述。

简而言之：**图像叙事 = 图像序列 + 生成故事文本**。最早比较广为人知的是 SIND / VIST（Sequential Image Narrative / Visual Storytelling Dataset）数据集，用来训练“从图像序列生成故事”模型。

==⚠ 主要挑战：==

1. **主题与情节的连贯性**：保证句子与句子之间有合理衔接，不是零散的描述。
2. **视觉-语言对齐**：每一句话需要和对应图片或图片间关系相关，不脱节。
3. **跨图像关系建模**：要理解图像序列中前后的视觉线索、时间顺序、场景变化。
4. **填补视觉空白**：图像之间可能存在“视觉空白”，模型需要“想象”中间情节（如 Hide-and-Tell 模型的思路）
5. **评价标准困难**：传统的 N-gram 匹配指标（BLEU, METEOR 等）往往和人类判断相关性差。为此出现了一些专门评估指标，如 RoViST 用于 visual storytelling 任务的评价（考虑视觉基础、连贯性、冗余性）

![3.4 Visual StoryTelling 案例 - (2016) 未公布代码](/cv/ImageGeneration/01_xxx/09_VS.jpeg =560x)

早期方法通常是把每张图用一个 caption 模型描述，再简单拼接 → 容易出现逻辑不连贯问题。后来引入注意力、主题建模、图像间关系建模等机制（例如“topic-aware storytelling”）来强化连贯性与一致性。最近的趋势是结合大模型 / 多模态预训练模型，让视觉 + 语言的表示更联合，生成更自然、更连贯的故事（如 VIST-GPT）

| 论文 / 方法                                                            | 主题 /贡献                                                        | 链接 / 来源              |
| ---------------------------------------------------------------------- | ----------------------------------------------------------------- | ------------------------ |
| _Visual Storytelling_（2016）                                          | 引入 SIND 数据集、定义视觉叙事任务，提出基线方法                  | arXiv / ACL ([arXiv][1]) |
| _RoViST: Learning Robust Metrics for Visual Storytelling_              | 针对 visual storytelling 设计的新评价方法（视觉、连贯性、非冗余） | arXiv ([arXiv][4])       |
| _TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling_ | 用强化学习 + 主题一致性来改进叙事质量                             | arXiv ([arXiv][5])       |
| _Openstory++: Instance-aware Open-domain Visual Storytelling_          | 引入实例级注释，处理开放域图像叙事问题                            | arXiv ([arXiv][6])       |

[1]: https://arxiv.org/abs/1604.03968?utm_source=chatgpt.com "[1604.03968] Visual Storytelling - arXiv"
[4]: https://arxiv.org/abs/2205.03774?utm_source=chatgpt.com "RoViST:Learning Robust Metrics for Visual Storytelling"
[5]: https://arxiv.org/abs/2403.11550?utm_source=chatgpt.com "TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling"
[6]: https://arxiv.org/abs/2408.03695?utm_source=chatgpt.com "Openstory++: A Large-scale Dataset and Benchmark for Instance ..."

### 3.3 视觉问答系统

==视觉问答系统（Visual Question Answering，VQA）== 是一种多模态理解任务，输入是一张图像（I）和一个关于该图像的自然语言问题（Q），模型需要基于视觉内容和语义推理，输出一个自然语言答案（A）。

VQA 需要同时理解图像（视觉特征，如物体、场景、关系）和文本（问题的语义、关键词）。主要挑战有：

1. **感知层面**：检测物体、属性和关系。
2. **语义层面**：理解自然语言问题，可能涉及实体、动作、推理。
3. **推理层面**：需要多步推理，例如“图片里有几只戴帽子的狗？”

![3.5 Visual Question Answering 案例 - vqa.pytorch model](/cv/ImageGeneration/01_xxx/10_vqa.png =560x)

它的应用也有不少，比如：在图像序列中推理潜在的安全风险 / 异常情况；在医学影像中辅助诊断（图 + 问 → 提供可能性），延伸一点就是人机交互中的图片问答小助手；社交媒体内容的审查或辅助理解；教育工具中辅助图像教材理解等等。

| 论文                                                                                                                | 核心创新 / 亮点                                                                                       |
| ------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| _Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering_ (2023) | 提出 CopVQA，通过分层因果推理提高模型在未见分布 (out-of-distribution) 上的泛化能力。([arXiv][1])      |
| _R-VQA: A robust visual question answering model_ (2024)                                                            | 强调语言偏差和组合逻辑（compositional reasoning）；改进模型在这些挑战上的鲁棒性。([ScienceDirect][4]) |
| _Visual question answering: from early developments to recent advances -- a survey_ (2025)                          | 最新 survey，涵盖最近 VQA 架构、数据集、评价指标与实际应用。([arXiv][5])                              |

[1]: https://arxiv.org/abs/2310.05410?utm_source=chatgpt.com "Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering"
[4]: https://www.sciencedirect.com/science/article/abs/pii/S0950705124014618?utm_source=chatgpt.com "R-VQA: A robust visual question answering model - ScienceDirect"
[5]: https://export.arxiv.org/abs/2501.03939?utm_source=chatgpt.com "[2501.03939] Visual question answering: from early developments to recent advances -- a survey"

## 4. 多模态 T2I

==文本到图像生成（Text-to-Image Generation，T2I）== 是计算机视觉和自然语言处理领域的一个重要研究方向，旨在根据自然语言描述生成对应的图像。该任务通常包括两个主要步骤：

1. 文本编码（Text Encoding）：将输入的自然语言描述转化为机器可理解的向量表示。
2. 图像生成（Image Generation）：根据文本的向量表示生成符合描述的图像。

#### 发展历程与关键技术：

最初的 T2I 方法主要基于条件生成对抗网络（Conditional GANs）和变分自编码器（VAE），如 StackGAN 和 AttnGAN。

近年来，扩散模型（Diffusion Models）成为主流，如 Stable Diffusion 和 DALL·E 2，它们在生成质量和多样性上取得了显著进展。结合视觉和语言信息的多模态方法，如 CLIP 和 Flamingo，进一步提升了生成的语义一致性和图像质量。

- **文本编码器**：如 BERT、CLIP、T5，用于将自然语言描述转化为向量表示。
- **图像生成模型**：如 GAN、VAE、扩散模型，用于生成图像。
- **多模态对齐**：通过对齐视觉和语言特征，提升生成图像的语义一致性。
- **控制机制**：如文本引导、风格控制、局部编辑等，增强生成图像的可控性。

在图像到文本（I2T）任务中，研究方向相对集中，可以较为清晰地划分为三个主要子任务。相比之下，文本到图像（T2I）任务的研究更加复杂与多样化，其目标不仅是生成与输入文本语义一致的图像，还涉及控制性、连贯性和创造性等更高层次的挑战。

因此，T2I 任务并不能像 I2T 那样简单划分为几个固定子任务，而是可以根据生成目标与控制方式，大致归纳为三类：

1. **全局生成类（Global Generation）**：关注根据整段文本生成完整图像，强调语义一致性与视觉逼真度。
2. **局部/受控生成类（Controlled Generation）**：在生成过程中引入控制机制（如布局、分割、草图、关键点、编辑指令等），以实现更细粒度的引导。
3. **序列/故事生成类（Sequential / Story Generation）**：扩展至图像序列或多模态故事生成，强调多张图像之间的语义连贯性与叙事逻辑。

### 4.1 全局生成类

### 4.2 局部/受控生成类

### 4.3 序列/故事生成类

## 5. 视频生成相关

视频任务本质上是 **图像生成的时序扩展**，所以很多子任务确实来源于图像领域，但额外要考虑 时间一致性（temporal consistency） 和 跨帧依赖（long-term dependency）。

### 5.1 生成与修复

视频生成 / 视频合成（Video Generation / Video Synthesis）

影视特效、虚拟人、动画生成。

视频修复（Video Inpainting / Video Restoration）是指修复视频中的缺失或损坏区域，保证时序一致性。

细分：视频去噪 / 去模糊（Video Denoising/Deblurring）；视频擦除 / 修复（Video Inpainting）

老影片修复、广告去除、遮挡去除。

### 5.2 超分与补帧

视频超分辨率（Video Super-Resolution, VSR）是指从低分辨率视频恢复高分辨率视频，同时保持时间一致性。

流媒体增强、老视频重制、监控视频优化。

视频补帧（Video Frame Interpolation, VFI）是指在已知相邻帧之间生成中间帧，提高视频帧率。

影视补帧（60fps）、慢动作生成。

### 5.3 预测与编辑

视频预测（Video Prediction / Future Frame Prediction）是指基于历史视频帧预测未来帧。

自动驾驶环境预测、监控预警。

视频编辑（Video Editing / Video-to-Video）是指对已有视频进行修改或重新渲染，同时保持运动一致性。

风格迁移（Video Style Transfer）视频重定向 / 换脸 / 换景（Video Retargeting / Face Swapping）条件编辑（文本驱动的视频编辑, Text-to-Video Editing）

影视后期、虚拟拍摄。

## 6. 3D 领域

3D 生成领域确实比图像/视频要新很多，但现在非常火，特别是 NeRF、3DGS 这些方法。

### 6.1 3D 重建

3D 重建（3D Reconstruction）是指从输入（图像、视频、深度图、点云）中恢复出物体或场景的 3D 几何形状。

- 单视图 3D 重建（Single-view 3D Reconstruction）：一张图恢复 3D（你说的“2D → 3D 手办”就是这个）。
- 多视图 3D 重建（Multi-view 3D Reconstruction）：多张不同角度图生成 3D。
- 基于视频的重建：用短视频恢复物体或场景的 3D 模型。

应用：虚拟人、手办、AR/VR 建模、影视特效。

### 6.2 3D 生成

3D 生成（3D Generation）是指直接生成新的 3D 内容，可以是形状、纹理、完整场景。

代表模型：DreamFusion（Google, 2022）、Magic3D（2022）、最近的 3D Gaussian Splatting + Diffusion。

应用：游戏资产生成、元宇宙内容、影视建模。

### 6.3 神经渲染

神经渲染（Neural Rendering）是指学习从 3D 场景生成逼真的 2D 渲染图像，或者反过来由 2D 监督优化 3D 表示。

- NeRF (Neural Radiance Fields)：用神经网络隐式表示场景。
- 3D Gaussian Splatting (3DGS)：比 NeRF 更快的新方法，2023 年爆火。

自由视角视频、虚拟摄影机、影视后期。

### 6.4 3D 编辑与操控

3D 编辑与操控（3D Editing & Manipulation）是指在已有 3D 模型/场景上进行修改，比如换材质、换姿态、编辑几何结构。

应用：虚拟换装、家具布置、电影场景编辑。

### 6.5 动态 3D 生成

动态 3D 生成（Dynamic 3D Generation）不仅生成静态 3D 模型，还要生成随时间变化的动态 3D（比如一个人跳舞的 3D 动作序列）。

应用：虚拟数字人、动画制作、动作捕捉替代方案。
