---
title: 图像生成综述
date: 2025/09/21
categories:
  - 计算机视觉
tags:
  - 图像生成
  - 综述
---

**图像生成（Image Generation）** 是计算机视觉与人工智能中的核心任务之一，旨在让机器==从数据分布中==学习视觉世界的规律，并能够生成符合这些规律的、具有逼真感与语义合理性的图像。它的核心要点有三层：

1. **分布学习**：图像生成并不是“凭空画图”，而是通过模型学习训练数据的统计分布。如今大多数的生成模型，从数学的角度上来看，都是概率与统计模型。模型在采样时，等价于“从学到的分布中抽取新的样本”。

2. **生成方式**：可以是 ==无条件生成==（仅依赖模型权重，随机噪声/隐变量等，采样得到图像）。也可以是 ==有条件生成==（在文本、标签、图像等条件引导下生成）。

3. **目标**：生成图像在 视觉质量（fidelity） 和 多样性（diversity） 上要与真实图像相当。同时逐步发展出 可控性（controllability） 和 跨模态性（cross-modality） 的能力，例如“文生图”、“图像编辑”。

对于不同的业务领域，图像生成还有很多的子任务划分，大致可以分类为以下几种：

## 1. 图像修复与复原

### 1.1 图像修复

==图像修复（Image Inpainting）== 并不是我们广义上理解的，所有缺陷都可以修复的。一般都是指 **基于 Mask 的内容补全**，不论这个图像是不是真的有残缺破损的内容缺失，我们都把需要修复的部分盖上一个 mask，当作这一部分缺失了，然后让模型进行推测和补全，这就是 Inpainting。

![1.1 Inpainting 案例 - Latent Diffusion Model](/cv/ImageGeneration/01_xxx/01_inpainting.png =560x)

我在制作我的数据集和读论文的过程中，发现还有一小类是归属于 Inpainting 范畴的，它是 ==色彩还原==。这个部分是说图像的纹理和结构都是完整的清晰的，但是出现了褪色、染色、发霉等情况，需要对色彩、亮度、对比度进行恢复或统一。

比如老照片的褪色修复，黑白图像上色（Colorization），色彩迁移这些。另外，去水印目前的技术一般都是基于 mask 的修复，但是我觉得有一类水印是可以归于色彩矫正的，**透明水印**。它只是文字或者 Logo 的周围有极小的阴影扰动，底层的纹理还是清晰可见的，但目前我并没有发现通过色彩校正的方式来消除透明水印的论文或方法。

![1.2 Colorization 案例 - DDColor Model](/cv/ImageGeneration/01_xxx/02_teaser.webp =560x)

### 1.2 图像复原

==图像复原/重建（Image Restoration）== 是一个更大范围的词，有的综述里把 Inpainting 也归于复原当中，但这里我们只说 **去噪、去模糊、超分辨率**。

关于==去噪（Denoising）==，我在《图像处理》中已经讲过噪声的来源，它的目的是移除这些噪声的同时，保持原始图像的纹理细节。传统方法多依赖滤波或稀疏表示，而深度学习方法则通过学习大规模数据分布，能够更好地在“去除噪声”与“保留细节”之间取得平衡。

![1.3 Denoising 案例 - noise2noise Model](/cv/ImageGeneration/01_xxx/03_denoise.png =560x)

**模糊图像**常来源于运动（如相机抖动、物体快速移动）或光学散焦，导致图像丢失高频细节。==去模糊（Deblurring）== 的目标是反卷积并恢复清晰的结构信息。早期方法基于物理模型估计模糊核，而现代深度模型则通过端到端学习完成去模糊，能够生成更自然、锐利的图像。

![1.4 Deblurring 案例 - DeblurDiNAT Model](/cv/ImageGeneration/01_xxx/04_deblurring.jpeg =560x)

==超分辨率（Super-Resolution, SR）== 旨在将低分辨率图像恢复为高分辨率图像。它不仅要求增加像素数量，还需要生成合理的细节与纹理。OpenCV 中的 Resize 是通过数学插值的方法来猜测出更多的像素，本质上没办法创造出丢失的细节，所以放大后的图像看起来模糊，缺乏高频细节。近年来基于深度学习的方法（如 SRCNN、ESRGAN）在自然图像、医学影像和卫星遥感中均取得了显著突破。

![1.5 Super-Resolution 案例 - Real-ESRGAN Model](/cv/ImageGeneration/01_xxx/05_sr.jpg =560x)

## 2. 图像风格与域转换

### 2.1 图像风格迁移

==图像风格迁移（Style Transfer）== 是指在保持原始图像内容和结构的前提下，将另一幅图像的艺术风格或视觉效果迁移过来。重点在于：**内容保持一致 → 风格发生改变。**

最早的代表是 神经风格迁移（Neural Style Transfer, 2016），通过 CNN 的特征分离，保留“内容特征”，替换“风格特征”。后续工作逐渐扩展到 快速风格迁移（Fast Style Transfer）、任意风格迁移（Arbitrary Style Transfer），甚至是 高分辨率 / 视频风格迁移。

近年来，Diffusion 与生成模型的发展，使得风格迁移不再局限于“梵高画风 → 照片”，而是可以在 色彩、纹理、光照、甚至领域（domain） 上进行灵活迁移，例如“照片转油画”、“卡通化”、“低光增强+风格化”。

![2.1 Style Transfer 案例 - visual-concept-translator Model](/cv/ImageGeneration/01_xxx/06_style_transfer.jpg =560x)

### 2.2 图像翻译

==图像翻译（I2I Translation）== 与风格迁移密切相关，但目标更广泛。它指的是**将一张输入图像转换为另一种表现形式，不仅仅是风格变化，还包括语义、域、模态的跨域转换**。典型例子包括：“素描 → 照片”、“标签图 → 场景”、“马 ↔ 斑马”、“夏天 ↔ 冬天”。

随着扩散模型和注意力机制的发展，近年 I2I 可以做到更高层次的**可控生成**。比如：条件引导（文本 + 图像），实现 文本驱动的图像翻译；保持输入图像结构，改变域或属性，例如“白天 → 夜晚”、“真实人像 → 动漫人物”。

![2.2 I2I Translation 案例 - CycleGAN Model](/cv/ImageGeneration/01_xxx/07_i2i_translation.jpg =560x)

风格迁移任务和翻译任务之间，对比下来可以这么理解：**风格迁移**是 I2I 的一个特殊子任务（内容不变，风格变）。**图像翻译**则涵盖了更多变化，比如 语义域改变、结构迁移、模态切换。

补充说明：I2I 是 Image-to-Image 的缩写，表示图像到图像任务，之前说的图像的修复与复原，包括现在的风格迁移与翻译，大多数的模型都是 I2I 任务，有少部分比较新的模型，会涉及多模态领域，会有 **文本到图像（Text-to-Image）** 与 **图像到文本（Image-to-text）** --> T2I、I2T 这样的缩写词。

## 3. 多模态（图文交融）

### 3.1 文本到图像

AIGC 核心：Stable Diffusion、DALL·E、Imagen

可扩展到 跨模态生成：Prompt + 图像 → 新图像

### 3.2 图像字幕（I2T）

图像字幕（Image Captioning）

### 3.3 图像叙事（I2T）

图像叙事（Image StoryTelling）

## 4. 视频生成相关

视频预测（给几帧生成未来帧）

文本到视频（Text-to-Video）

## 5. 3D 领域

文本/图像 → 3D 场景（NeRF、DreamFusion）
