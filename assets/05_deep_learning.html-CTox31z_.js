import{_ as g,c as a,b as t,e as l,a as i,w as e,r as m,o,d as p}from"./app-P8O63--9.js";const r="/blog-vp-reco/machinelearning/two/05-1.png",d="/blog-vp-reco/machinelearning/two/05-2.png",c="/blog-vp-reco/machinelearning/two/05-3.png",h="/blog-vp-reco/machinelearning/two/05-4.png",u="/blog-vp-reco/machinelearning/two/05-10.png",f="/blog-vp-reco/machinelearning/two/05-11.png",_="/blog-vp-reco/machinelearning/two/05-12.png",y="/blog-vp-reco/machinelearning/two/05-6.png",v="/blog-vp-reco/machinelearning/two/05-13.png",b="/blog-vp-reco/machinelearning/two/05-14.png",w="/blog-vp-reco/machinelearning/two/05-15.png",x="/blog-vp-reco/machinelearning/two/05-16.png",k="/blog-vp-reco/machinelearning/two/05-5.png",A="/blog-vp-reco/machinelearning/two/05-7.png",I="/blog-vp-reco/machinelearning/two/05-8.png",z="/blog-vp-reco/machinelearning/two/05-9.png",T="/blog-vp-reco/machinelearning/two/06-1.png",G="/blog-vp-reco/machinelearning/two/06-2.png",M="/blog-vp-reco/machinelearning/two/06-3.png",N="/blog-vp-reco/machinelearning/two/06-4.png",S="/blog-vp-reco/machinelearning/two/06-5.png",P={},B={class:"katex-block"},R={class:"katex-display"},q={class:"katex"},L={class:"katex-html","aria-hidden":"true"},U={class:"base"},H={class:"mord accent"},V={class:"vlist-t"},Y={class:"vlist-r"},C={class:"vlist",style:{height:"0.714em"}},O={style:{top:"-3em"}},j={class:"accent-body",style:{left:"-0.1522em"}},E={class:"overlay",style:{height:"0.714em",width:"0.471em"}},J={xmlns:"http://www.w3.org/2000/svg",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},Z={class:"base"},D={class:"mord accent"},F={class:"vlist-t"},K={class:"vlist-r"},Q={class:"vlist",style:{height:"0.714em"}},W={style:{top:"-3em"}},X={class:"accent-body",style:{left:"-0.2077em"}},$={class:"overlay",style:{height:"0.714em",width:"0.471em"}},ss={xmlns:"http://www.w3.org/2000/svg",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},ts={class:"base"},is={class:"mord"},ns={class:"mfrac"},as={class:"vlist-t vlist-t2"},ls={class:"vlist-r"},es={class:"vlist",style:{height:"1.3214em"}},os={style:{top:"-2.296em"}},ps={class:"mord"},rs={class:"mord"},gs={class:"msupsub"},ms={class:"vlist-t"},ds={class:"vlist-r"},cs={class:"vlist",style:{height:"0.814em"}},hs={style:{top:"-2.989em","margin-right":"0.05em"}},us={class:"sizing reset-size6 size3 mtight"},fs={class:"mord mtight"},_s={class:"mord accent mtight"},ys={class:"vlist-t"},vs={class:"vlist-r"},bs={class:"vlist",style:{height:"0.714em"}},ws={style:{top:"-2.714em"}},xs={class:"accent-body",style:{left:"-0.1522em"}},ks={class:"overlay mtight",style:{height:"0.714em",width:"0.471em"}},As={xmlns:"http://www.w3.org/2000/svg",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},Is={class:"mord accent mtight"},zs={class:"vlist-t"},Ts={class:"vlist-r"},Gs={class:"vlist",style:{height:"0.714em"}},Ms={style:{top:"-2.714em"}},Ns={class:"accent-body",style:{left:"-0.2077em"}},Ss={class:"overlay mtight",style:{height:"0.714em",width:"0.471em"}},Ps={xmlns:"http://www.w3.org/2000/svg",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"};function Bs(Rs,s){const n=m("font");return o(),a("div",null,[s[49]||(s[49]=t("p",null,"神经网络初探（Neural network model）",-1)),s[50]||(s[50]=t("h2",{id:"_1-深度学习的起源和发展",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#_1-深度学习的起源和发展"},[t("span",null,"1.深度学习的起源和发展")])],-1)),s[51]||(s[51]=t("h3",{id:"_1-1-发展历程",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#_1-1-发展历程"},[t("span",null,"1.1 发展历程")])],-1)),l(n,{color:"green"},{default:e(()=>[...s[0]||(s[0]=[p("Origin：Algorithms that try to mimic the brain. ",-1)])]),_:1}),s[52]||(s[52]=t("p",null,"在几十年前首次发明神经网络时，研究人员的动机是编写一个软件，可以模仿人脑或生物大脑，解析它们是如何学习和思考的。",-1)),l(n,{color:"green"},{default:e(()=>[...s[1]||(s[1]=[p("Used in the 1980's and early 1990's.But fell out of favor in the late 1990's.",-1)])]),_:1}),s[53]||(s[53]=t("p",null,"80 年代以及 90 年代初期，在手写数字识别等应用程序中有很大的新引力，甚至被用来读取邮政编码以进行路由分发信件，以及读取支票中的美元数字。但是在 90 年代末期逐渐失去了市场。",-1)),l(n,{color:"green"},{default:e(()=>[...s[2]||(s[2]=[p("Resurgence from around 2005. ",-1)])]),_:1}),s[54]||(s[54]=i('<p>于 2005 年开始复苏，大概是这个时候被用深度学习重新命名。也就是从这时起，神经网络在一个又一个领域内进行了覆盖式的革新。比如：语音处理（Speech）--&gt; 图像识别（Images）--&gt; 自然语言处理（Text/NLP）--&gt; 气候变换、医学成像、在线广告、推荐系统等各个方面。</p><p><mark>所以说，深度学习其实就是神经网络模型版本的机器学习。</mark></p><h3 id="_1-2-神经元和大脑" tabindex="-1"><a class="header-anchor" href="#_1-2-神经元和大脑"><span>1.2 神经元和大脑</span></a></h3><p>到了今天，即使（人工）神经网络已经变得与我们曾经认为的「大脑实际在工作中的学习方式」不同了，但一些生物学动机仍然存在于我们的思维方式中，所以我们先来看看大脑是如何工作的，以及它与神经网络之间的关系。</p><figure><img src="'+r+'" alt="5.1 神经元和大脑" width="560" tabindex="0" loading="lazy"><figcaption>5.1 神经元和大脑</figcaption></figure><p>回忆一下初中生物知识：一个神经元由一个细胞体（cell body）组成， 一个神经元会接收很多信号（也就是输入 input），接收信号的部位叫做树突（dendrites），然后神经元也会偶尔通过轴突（axon）向其他的神经元发送电脉冲信号（也就是输出 output），在接收的神经元中，这个电脉冲信号就是输入，这就构成了神经网络的层级结构。</p><p>实际上神经学家一直在不断地探索人脑是如何构成的，我们现在所使用的神经网络模型也仅仅只是简单的模拟的人脑构造，随着科技的进步，未来肯定还会有变革出现。在目前我们通过这些极其简化的神经元模型，也能构建出一些很强大的深度学习算法。</p><p>当然了，也不用太过在意生物学的理论系统，在深度学习领域，我们更多地是使用工程原理来探讨如何构建更有效的算法。</p><h3 id="_1-3-快速兴起的原因" tabindex="-1"><a class="header-anchor" href="#_1-3-快速兴起的原因"><span>1.3 快速兴起的原因</span></a></h3><p>为什么神经网络在 05 年之后才开始飞速发展？这其实是跟数字化、网络化的时代发展息息相关的，现在不管是医疗领域、社交领域、购物领域等等，都从纸质的记录变成了网络上的数据，数据的数量已经指数级增长了。</p><p>假设我们现在有 1 亿份的数据，传统的线性回归，逻辑回归这样的机器学习，在 100 万数据之后，它的预测能力是不会有太大变化的，它无法有效的利用大量数据。而神经网络模型，小型、中型、大型， 它们可能在 1000 万数据、5000 万数据、9000 万数据的处理中，预测的更加精确，它可以有效的利用起来海量数据。这就是为什么神经模型模型突然兴起的原因。</p><p>这同样也是为什么更快的计算机处理，包括 GPU 或图形处理器芯片的兴起。GPU 最早只是为了生成更优雅的计算机图形，但同样它对机器学习来说也非常强大。这就是神经网络的起源和发展。</p><h2 id="_2-模拟商品的需求预测" tabindex="-1"><a class="header-anchor" href="#_2-模拟商品的需求预测"><span>2.模拟商品的需求预测</span></a></h2><h3 id="_2-1-简单预测" tabindex="-1"><a class="header-anchor" href="#_2-1-简单预测"><span>2.1 简单预测</span></a></h3><p>假设我们需要预测一个品牌的 T 恤是否畅销，这可能关系到展业链的投资、股票起伏等等，有助于我们发现商机获得利益。</p><figure><img src="'+d+'" alt="5.2 简单需求预测" width="560" tabindex="0" loading="lazy"><figcaption>5.2 简单需求预测</figcaption></figure><p>看途中左边的坐标系，我们会发现这是一个分类问题，用逻辑回归去处理对吧。在这个模型中，它的输入是 T 恤的价格，输出就是逻辑回归函数。</p>',17)),l(n,{color:"blue"},{default:e(()=>[...s[3]||(s[3]=[p("在机器学习中，我们把输出写做 f(x)，为了向神经网络转变，我们将其写为 a，它是 activation 的缩写，表示激活。这个单词它源于神经科学的术语，是指一个神经元向下游的其他神经元发送了多少高电平信号。",-1)])]),_:1}),s[55]||(s[55]=i('<p>实际上这个逻辑回归函数，它可以看作一个非常小的神经网络，只有一个神经元，这个神经元中做的事情，就是通过接收 T 恤的 price，通过计算，输出畅销与否的概率预测值。</p><h3 id="_2-2-复杂预测" tabindex="-1"><a class="header-anchor" href="#_2-2-复杂预测"><span>2.2 复杂预测</span></a></h3><p>一个商品畅销与否当然不可能只有 “价格” 这一个因素存在，我们稍微完善一下，假设畅销与否在提供者层面和四个因素（特征值）有关，它们分别是：</p><ol><li>售卖价格（price）</li><li>运输成本（shipping cost）</li><li>市场营销（marketing）</li><li>材料质量（material）</li></ol><p>在消费者层面，你可能会怀疑一件 T 恤是否能成为畅销品，取决于这几个因素：</p><ol><li>对于大众而言这个 T 恤价格是否负担的起（affordability）</li><li>潜在买家对于这款 T 恤的认知度如何（awareness）</li><li>感官感知导致的偏见或潜在偏见（perceived quality）</li></ol><p>我们要做的是创建一个人工神经网络来尝试估计这件 T 恤被认为非常实惠的概率。</p><figure><img src="'+c+'" alt="5.3 复杂需求预测" width="560" tabindex="0" loading="lazy"><figcaption>5.3 复杂需求预测</figcaption></figure><p>第一，支付能力主要是价格和运费的函数，因为支付的总额是商品价格加上运费的一部分。我们将在这里使用一个小的神经元，一个逻辑回归单元来输入价格和运输成本，并预测人们是否认为这是负担的起的。（第一个蓝色圆圈）</p><p>第二，我们再使用另一个神经元来估计一下，买家对于这款 T 恤的认知度如何，在这种情况下，知名度主要是靠 T 恤的市场营销。（第二个蓝色圆圈）</p><p>第三，最后，要创建一个神经元来估计人们是否认为这是高质量的，这可能取决于 T 恤的价格和材料质量。为什么会有价格，是因为人们有时会觉得，一分钱一分货，价格高的质量就好。（第三个蓝色圆圈）</p><p>这三种估计的方式，affordability、awareness、perceived quality，我们将其称为三个神经元（蓝色圆圈），同时连接到另一个神经元（粉色圆圈），这是另一个逻辑回归的的算法，通过获取蓝色神经元的三个输入数据，输出这件 T 恤是否畅销的概率。</p><h3 id="_2-3-神经网络术语" tabindex="-1"><a class="header-anchor" href="#_2-3-神经网络术语"><span>2.3 神经网络术语</span></a></h3><p>我们将蓝色的三个神经元分为一个组，将粉色的神经元分为一个组，形成所谓的神经网络层（layer）。一个神经网络层可以有一个神经元或多个神经元。</p><figure><img src="'+h+'" alt="5.4 神经网络术语" width="560" tabindex="0" loading="lazy"><figcaption>5.4 神经网络术语</figcaption></figure><p>黄色的初始数据叫做输入层，简单的来看就是一个拥有四个值的数组，当然，为了简化计算的复杂度，我们可以将其改为一个四元向量。</p><p>粉色的神经网络层叫做输出层，因为它的输出结果就是神经网络预测的畅销概率，也就是我们要的最终结果。</p><p>蓝色的神经网络层可以称做隐藏层，因为这样的一个模型，你知道通过输入的值（输入层）可以得到结果（输出层），但是你的数据集并没有告诉你什么是 affordability、awareness、perceived quality（中间层的输出），你在训练集中看不到它们，所以中间层被称为隐藏层。</p>',18)),l(n,{color:"blue"},{default:e(()=>[...s[4]||(s[4]=[p("隐藏层生成的 affordability、awareness、perceived quality 这三个输出值也叫做激活。激活这个名字来自于神经科学领域，它指的是生物神经元向其下游的其他神经元发送高电平或者发送电脉冲的程度。同样粉色的神经元输出的概率值，也称作粉色神经元的激活。",-1)])]),_:1}),s[56]||(s[56]=i('<h2 id="_3-向前传播算法-经典架构" tabindex="-1"><a class="header-anchor" href="#_3-向前传播算法-经典架构"><span>3.向前传播算法（经典架构）</span></a></h2><h3 id="_3-1-神经网络层解析" tabindex="-1"><a class="header-anchor" href="#_3-1-神经网络层解析"><span>3.1 神经网络层解析</span></a></h3><p>大多数现代神经网络的基本构建块是一层神经元，在这节中，你将学习如何构建神经元层，一旦掌握了神经元层，你就可以将这些模块组合起来形成一个大的神经网络模型。</p><p>回想一下 T 恤的复杂预测模型，我们输入层的四个变量分别是，售卖价格、运输成本、市场营销和材料质量。我们将其输入到隐藏层之后，每一个神经元都是一个小的逻辑回归单元……</p><figure><img src="'+u+'" alt="5.5 T恤-输入层到隐藏层" width="560" tabindex="0" loading="lazy"><figcaption>5.5 T恤-输入层到隐藏层</figcaption></figure><p>回顾复习一下以前的知识：g(z) 代表逻辑回归，括号中的 z 代表线性回归，计算完 z，将结果带入我们的 sigmoid 函数，也就是逻辑回归函数里，就得到了一个神经元的结果。</p>',6)),t("p",B,[t("span",R,[t("span",q,[s[48]||(s[48]=i('<span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>⃗</mo></mover><mo>⋅</mo><mover accent="true"><mi>x</mi><mo>⃗</mo></mover><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>⃗</mo></mover><mo>⋅</mo><mover accent="true"><mi>x</mi><mo>⃗</mo></mover><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">g(z)=g(\\vec{w}\\cdot\\vec{x}+b)=\\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x}+b)}} </annotation></semantics></math></span>',1)),t("span",L,[s[46]||(s[46]=i('<span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span>',1)),t("span",U,[s[8]||(s[8]=t("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}},null,-1)),s[9]||(s[9]=t("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g",-1)),s[10]||(s[10]=t("span",{class:"mopen"},"(",-1)),t("span",H,[t("span",V,[t("span",Y,[t("span",C,[s[7]||(s[7]=t("span",{style:{top:"-3em"}},[t("span",{class:"pstrut",style:{height:"3em"}}),t("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w")],-1)),t("span",O,[s[6]||(s[6]=t("span",{class:"pstrut",style:{height:"3em"}},null,-1)),t("span",j,[t("span",E,[(o(),a("svg",J,[...s[5]||(s[5]=[t("path",{d:`M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z`},null,-1)])]))])])])])])])]),s[11]||(s[11]=t("span",{class:"mspace",style:{"margin-right":"0.2222em"}},null,-1)),s[12]||(s[12]=t("span",{class:"mbin"},"⋅",-1)),s[13]||(s[13]=t("span",{class:"mspace",style:{"margin-right":"0.2222em"}},null,-1))]),t("span",Z,[s[17]||(s[17]=t("span",{class:"strut",style:{height:"0.7973em","vertical-align":"-0.0833em"}},null,-1)),t("span",D,[t("span",F,[t("span",K,[t("span",Q,[s[16]||(s[16]=t("span",{style:{top:"-3em"}},[t("span",{class:"pstrut",style:{height:"3em"}}),t("span",{class:"mord mathnormal"},"x")],-1)),t("span",W,[s[15]||(s[15]=t("span",{class:"pstrut",style:{height:"3em"}},null,-1)),t("span",X,[t("span",$,[(o(),a("svg",ss,[...s[14]||(s[14]=[t("path",{d:`M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z`},null,-1)])]))])])])])])])]),s[18]||(s[18]=t("span",{class:"mspace",style:{"margin-right":"0.2222em"}},null,-1)),s[19]||(s[19]=t("span",{class:"mbin"},"+",-1)),s[20]||(s[20]=t("span",{class:"mspace",style:{"margin-right":"0.2222em"}},null,-1))]),s[47]||(s[47]=i('<span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span>',1)),t("span",ts,[s[45]||(s[45]=t("span",{class:"strut",style:{height:"2.1088em","vertical-align":"-0.7873em"}},null,-1)),t("span",is,[s[43]||(s[43]=t("span",{class:"mopen nulldelimiter"},null,-1)),t("span",ns,[t("span",as,[t("span",ls,[t("span",es,[t("span",os,[s[39]||(s[39]=t("span",{class:"pstrut",style:{height:"3em"}},null,-1)),t("span",ps,[s[35]||(s[35]=t("span",{class:"mord"},"1",-1)),s[36]||(s[36]=t("span",{class:"mspace",style:{"margin-right":"0.2222em"}},null,-1)),s[37]||(s[37]=t("span",{class:"mbin"},"+",-1)),s[38]||(s[38]=t("span",{class:"mspace",style:{"margin-right":"0.2222em"}},null,-1)),t("span",rs,[s[34]||(s[34]=t("span",{class:"mord mathnormal"},"e",-1)),t("span",gs,[t("span",ms,[t("span",ds,[t("span",cs,[t("span",hs,[s[33]||(s[33]=t("span",{class:"pstrut",style:{height:"2.7em"}},null,-1)),t("span",us,[t("span",fs,[s[27]||(s[27]=t("span",{class:"mord mtight"},"−",-1)),s[28]||(s[28]=t("span",{class:"mopen mtight"},"(",-1)),t("span",_s,[t("span",ys,[t("span",vs,[t("span",bs,[s[23]||(s[23]=t("span",{style:{top:"-2.714em"}},[t("span",{class:"pstrut",style:{height:"2.714em"}}),t("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02691em"}},"w")],-1)),t("span",ws,[s[22]||(s[22]=t("span",{class:"pstrut",style:{height:"2.714em"}},null,-1)),t("span",xs,[t("span",ks,[(o(),a("svg",As,[...s[21]||(s[21]=[t("path",{d:`M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z`},null,-1)])]))])])])])])])]),s[29]||(s[29]=t("span",{class:"mbin mtight"},"⋅",-1)),t("span",Is,[t("span",zs,[t("span",Ts,[t("span",Gs,[s[26]||(s[26]=t("span",{style:{top:"-2.714em"}},[t("span",{class:"pstrut",style:{height:"2.714em"}}),t("span",{class:"mord mathnormal mtight"},"x")],-1)),t("span",Ms,[s[25]||(s[25]=t("span",{class:"pstrut",style:{height:"2.714em"}},null,-1)),t("span",Ns,[t("span",Ss,[(o(),a("svg",Ps,[...s[24]||(s[24]=[t("path",{d:`M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z`},null,-1)])]))])])])])])])]),s[30]||(s[30]=t("span",{class:"mbin mtight"},"+",-1)),s[31]||(s[31]=t("span",{class:"mord mathnormal mtight"},"b",-1)),s[32]||(s[32]=t("span",{class:"mclose mtight"},")",-1))])])])])])])])])])]),s[40]||(s[40]=i('<span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span>',2))]),s[41]||(s[41]=t("span",{class:"vlist-s"},"​",-1))]),s[42]||(s[42]=t("span",{class:"vlist-r"},[t("span",{class:"vlist",style:{height:"0.7873em"}},[t("span")])],-1))])]),s[44]||(s[44]=t("span",{class:"mclose nulldelimiter"},null,-1))])])])])])]),s[57]||(s[57]=i('<p>我们用 [num] 这种 <strong>上标</strong> 的形式来表示神经网络模型中的层相关的内容。在这里 [0] 就表示输入层，[1] 表示隐藏层，[2] 表示输出层</p><div class="layout"><figure><img src="'+f+'" alt="5.6 T恤-隐藏层到输出层" width="360" tabindex="0" loading="lazy"><figcaption>5.6 T恤-隐藏层到输出层</figcaption></figure><figure><img src="'+_+'" alt="5.7 T恤-输出层条件分支" width="360" tabindex="0" loading="lazy"><figcaption>5.7 T恤-输出层条件分支</figcaption></figure></div><p>通过这个案例，我们发现神经网络模型中的神经元，全部都是由逻辑回归单元构成的。解决分类问题，最后必不可少的一个环节就是判断，到底是 1 还是 0。此处只要大于 0.5，就表示 T 恤是畅销的。</p><h3 id="_3-2-多层神经网络解析" tabindex="-1"><a class="header-anchor" href="#_3-2-多层神经网络解析"><span>3.2 多层神经网络解析</span></a></h3><figure><img src="'+y+'" alt="5.8 多隐藏层" width="560" tabindex="0" loading="lazy"><figcaption>5.8 多隐藏层</figcaption></figure><p>当你构建自己的神经网络时，你需要做出的决定之一，是你想要多少个隐藏层，以及你希望每个隐藏层有多少个神经元。选择正确的隐藏层数，和每层隐藏神经元数，会对学习算法的性能产生影响。</p><p>这些问题是选择神经网络架构时的问题，在后面的章节会学习一些为神经网络选择合适架构的技巧。另外，在有的文献中，有人将这种具有多层的神经网络，叫做 <strong>多层感知器（multilayer perceptron）</strong>，是同一个东西。</p><figure><img src="'+v+'" alt="5.9 更复杂的神经网络" width="560" tabindex="0" loading="lazy"><figcaption>5.9 更复杂的神经网络</figcaption></figure><p>在多层的神经网络模型中，<strong>角标</strong> 是一个很容易被混淆的地方：</p><ol><li><strong>上标^num^</strong> 表示的是神经网络层的第几层，输入层是 [0] 层，第一个隐藏层是 [1] 层，然后依次类推往后排。</li><li><strong>下标~num~</strong> 表示的是同一层神经网络中的第几个神经元。</li><li><strong>向量 a</strong> 代表的是<mark>激活</mark>，也就是从上一层传递到下一层的值，所以上标就需要 -1。</li></ol><h3 id="_3-3-手写数字识别" tabindex="-1"><a class="header-anchor" href="#_3-3-手写数字识别"><span>3.3 手写数字识别</span></a></h3><p>让我们把学到的东西组合成一个算法，使得神经网络模型可以进行推理或预测。这里使用手写数字识别作为一个例子，为了简单起见，我们只写数字 0 和 1，所以这只是一个二进制分类问题，我们要输入图像并进行分类，分辨出这是 0 还是 1。</p><div class="layout"><figure><img src="'+b+'" alt="5.10 手写数字识别1" width="360" tabindex="0" loading="lazy"><figcaption>5.10 手写数字识别1</figcaption></figure><figure><img src="'+w+'" alt="5.11 手写数字识别2" width="360" tabindex="0" loading="lazy"><figcaption>5.11 手写数字识别2</figcaption></figure><figure><img src="'+x+'" alt="5.12 手写数字识别3" width="360" tabindex="0" loading="lazy"><figcaption>5.12 手写数字识别3</figcaption></figure></div><p>我们使用一个 8×8 像素的图像来对手写的数字进行切块，255 表示颜色最亮（没有写东西的白色），0 表示颜色最深（写了数字的黑色），中间值表示介于黑白之间的其他颜色。</p><p>介于只有 64 个特征值，我们只构建两个隐藏层，第一层有 25 个神经元，第二层有 15 个神经元。过程和前面的 T 恤预测基本一致。</p><p><mark>这种算法之所以被称作向前传播的算法，因为神经元的激活是从左到右顺序执行的。另外，这种类型的神经网络架构，最初有很多隐藏层和隐藏单元，随着靠近输出端，隐藏单元的数量会减少。是一种非常典型的架构。</mark></p><h2 id="_4-图像感知案例" tabindex="-1"><a class="header-anchor" href="#_4-图像感知案例"><span>4.图像感知案例</span></a></h2><h3 id="_4-1-特征构建-plus" tabindex="-1"><a class="header-anchor" href="#_4-1-特征构建-plus"><span>4.1 特征构建 Plus</span></a></h3><p>之前我们讲过特征工程这个内容，是说如果想要预测房子的价格，你可以用临街面也就是土地宽度 x1 和土地深度 x2，来构建一个更复杂的特征土地面积 x3。</p><figure><img src="'+k+'" alt="5.13 特征构建Plus" width="560" tabindex="0" loading="lazy"><figcaption>5.13 特征构建Plus</figcaption></figure><p>在特征工程中，需要我们查看已有的特征，并手动的决定如何将其组合在一起，以获得更好的特征。就像我们的 <strong>T 恤复杂预测案例</strong>，从四个 input，手动的构建了一个隐藏层，通过隐藏层计算出三个新的特征值。</p><p><mark>但真正的神经网络所做的是不需要你手动设计特征，它可以学习，自行计算出它想在这个隐藏层中使用的特征是什么，这就是神经网络成为很强大的学习算法的原因。</mark></p><h3 id="_4-2-人脸识别" tabindex="-1"><a class="header-anchor" href="#_4-2-人脸识别"><span>4.2 人脸识别</span></a></h3><p>如果你正在构建人脸识别的应用程序，你可能想要训练一个神经网络，将这样的图片作为输入，并输出图片中人的身份。</p><figure><img src="'+A+'" alt="5.14 人脸识别" width="560" tabindex="0" loading="lazy"><figcaption>5.14 人脸识别</figcaption></figure><p>这是一张 1000×1000 像素的图片，它在计算机中的表示实际上是一个 1000×1000 的网格，或者也称为像素强度值为 1000×1000 的矩阵。</p><p>在这个示例中，照片的像素强度值或像素亮度值范围在 0~255，因此矩阵中的 197 就是图片左上角的第一个像素的亮度值，203 就是图片第一行第三列的像素亮度值，同理，214 就是图片右下角像素亮度值。</p><p>如果你要获取这些强度值并将他们展开为一个向量，你最终会得到一个包含百万像素强度值的列表或向量（1000×1000）。人脸识别的问题是，你能不能训练一个神经网络，以百万像素亮度值的特征向量为输入，输出图片中人的身份。</p><h3 id="_4-3-观察隐藏层" tabindex="-1"><a class="header-anchor" href="#_4-3-观察隐藏层"><span>4.3 观察隐藏层</span></a></h3><figure><img src="'+I+'" alt="5.15 人脸识别隐藏层工作" width="560" tabindex="0" loading="lazy"><figcaption>5.15 人脸识别隐藏层工作</figcaption></figure><p>如果你尝试观察隐藏层的工作，你可能会发现：</p><ol><li>第一层神经元好像在提取很短的，具有不同线条的像素块。</li><li>第二层神经元好像在尝试分辨什么是眼睛、什么是鼻子、什么是耳朵。</li><li>第三层神经元正在聚合面部的不同部分，在尝试寻找较大的、还相对比较粗糙的面部形状。</li><li>最后一层神经元在对比图片中的脸，与对应不同脸型的匹配程度，包含非常丰富的功能，以帮助输出层确定人物图片的身份。</li></ol><p>你会惊讶的发现，在不同隐藏层的特征检测器都是单独的，在这个例子中，没有人告诉计算机要在第一层寻找短边，要在第二层寻找眼睛鼻子和面容等等。神经网络能从数据中自行找出这些东西。</p><p>事实上，我们发现神经网络在逐渐的扩大像素的大小区域，刚开始可能 1×1 的像素的区域，逐渐 10×10，然后 100×100，对应图像中的像素大小区域。</p><h3 id="_4-4-识别汽车" tabindex="-1"><a class="header-anchor" href="#_4-4-识别汽车"><span>4.4 识别汽车</span></a></h3><figure><img src="'+z+'" alt="5.16 汽车识别" width="560" tabindex="0" loading="lazy"><figcaption>5.16 汽车识别</figcaption></figure><p>做一个尝试，我们将数据集转化为很多张的汽车图片，要求用相同的算法检测汽车，我们会发现第一个隐藏层同样是在寻找短线条，第二个隐藏层开始组合车的不同零件，第三个隐藏层开始寻找更完整的汽车形状。</p><p>因此我们只需要向其提供不同的数据，神经网络就会自动学习检测不同的特征，以尝试进行汽车检测、人脸识别，或其他特定的事物的模型训练。</p><h2 id="_5-吴恩达教授的小故事" tabindex="-1"><a class="header-anchor" href="#_5-吴恩达教授的小故事"><span>5.吴恩达教授的小故事</span></a></h2><h3 id="_5-1-ani-与-agi" tabindex="-1"><a class="header-anchor" href="#_5-1-ani-与-agi"><span>5.1 ANI 与 AGI</span></a></h3><p>神经网络与 AI 或 AGI 之间的关系是什么？这是一个很有争议的话题，但因为它被广泛讨论，吴恩达老师想和大家分享一下自己的一些看法。<strong>这一节的内容 “我” 代表 “吴恩达教授”。</strong></p><p>我从十多岁开始使用神经网络时，就梦想有一天是否可以收获一个，和人类一样聪明的 AI，这条路十分的困难。让我们来看看 AGI，通用人工智能的梦想是怎么样的，并推测一下有朝一日到达那里的可能路径、不明确路径和困难路径。</p><p>我认为关于 AGI 或者说通用人工智能有很多不必要的炒作，也许其中有一个原因是人工智能实际上包括两种截然不同的东西。</p><figure><img src="'+T+'" alt="5.17 ANI 与 AGI" width="560" tabindex="0" loading="lazy"><figcaption>5.17 ANI 与 AGI</figcaption></figure><p>第一种是 ANI，这是一个只做一件事情的 AI 系统，一项狭窄的任务，有时做的好可能就会非常有价值，比如智能扬声器或自动驾驶汽车、网络搜索、应用于农业或工厂等的特定应用的 AI。</p><p>在过去的几年里，ANI 取得了巨大的进步，正如我们听到过的新闻信息，它正在为当今世界创造巨大的价值。 由于 ANI 是 AI 的一部分，它的快速发展让 AI 在过去的十年里也取得了巨大的进步，这样一个逻辑也是说的通的。</p><p>第二种是 AGI，希望构建出来可以做，任何普通人能够做的事儿的人工智能系统。尽管 ANI 取得了很大的进展，但是我并不确定我们在 AGI 方面真正取得了多少进展。</p><p>我认为 ANI 的所有进步都让人们认为 AI 取得了巨大的进步，这是一个错误的论点，因为 AI 的大量进步必然意味着 AGI 的大量进步。所以，看着 AI 的这张图，会有助于解释 AI 中发生的一些事情，以及一些关于 AGI 不必要的炒作来源。</p><h3 id="_5-2-agi-发展的困难途径" tabindex="-1"><a class="header-anchor" href="#_5-2-agi-发展的困难途径"><span>5.2 AGI 发展的困难途径</span></a></h3><p>神经网络出现时，我们就开始模拟神经元。随着现代深度学习的兴起，越来越快的计算机，甚至是 GPU，我们可以模拟更多的神经元。</p><p>我在很多年前就有这样一个大的期望，天哪，如果我们能模拟很多神经元，那么我们就可以模拟人脑或类似人脑的东西，我们就会拥有真正的智能系统。可悲的是，事实并非如此。我认为这有两个原因。</p><figure><img src="'+r+'" alt="5.18 神经元和大脑" width="560" tabindex="0" loading="lazy"><figcaption>5.18 神经元和大脑</figcaption></figure><p>首先，看着咱们正在构建的人工神经网络，逻辑回归单元实际上与任何生物神经元所做的完全不同，它比任何的生物神经元都简单太多了。</p><figure><img src="'+G+'" alt="5.19 生物神经元的内核" width="560" tabindex="0" loading="lazy"><figcaption>5.19 生物神经元的内核</figcaption></figure><p>其次，即使到了今天，我认为我们还是不知道大脑到底是如何工作的。比如关于神经元究竟如何从输入映射到输出，我们今天仍旧不知道。所以我们现在对人脑尝试的复杂模拟，与人脑实际行为的准确模型都有点相去甚远，更别说单个的逻辑函数了。</p><p>鉴于我们现在以及可能在不久的将来，对人脑如何工作的理解还非常有限，所以我认为仅仅试图模拟人脑作为通向 AGI 的途径，将是一条极其困难的道路。</p><h3 id="_5-3-agi-突破的可能路径" tabindex="-1"><a class="header-anchor" href="#_5-3-agi-突破的可能路径"><span>5.3 AGI 突破的可能路径</span></a></h3><p>话虽如此，有没有希望在我们的有生之年看到通用人工智能的突破呢？让我与你分享一些证据，这些证据可以帮助我保持这种希望，至少对于我自己而言。</p><p>科学家们已经在动物身上进行了一些有趣的实验，这些实验表明同一块生物的脑组织可以完成令人惊讶的范围广泛的任务。这导致一种关于学习算法的假设产生，即很多智能可能归因于一种或少数几种学习算法。如果我们能弄清那一个或一小部分算法是什么，我们也许有一天能够在计算机中实现它。</p><div class="layout"><figure><img src="'+M+'" alt="5.20 脑组织听觉实验" width="360" tabindex="0" loading="lazy"><figcaption>5.20 脑组织听觉实验</figcaption></figure><figure><img src="'+N+'" alt="5.21 脑组织触觉实验" width="360" tabindex="0" loading="lazy"><figcaption>5.21 脑组织触觉实验</figcaption></figure></div><p>来分享一些特别的实验。这是一个叫 Roy 的大脑，图 5.20 中黄色的部分显示的是大脑的听觉皮层，你的耳朵听到声音信号，然后你的大脑以电脉冲的形式从你的耳朵里接收信号。实验证明，如果你重新连接动物大脑，也就是切断耳朵和听觉皮层之间的导线，将图像反馈送到听觉皮层，那么听觉皮层就会学会看东西。</p><p>图 5.21 这是另一个例子，你大脑的这一部分是你的体感皮层，体感就是触摸的感觉。如果你类似的重新连接大脑，切断触摸传感器与大脑部分的连接，重新连接视觉信号，那么体感皮层就会学会看。</p><p>有这一系列的实验，表明大脑的许多不同的部分，仅仅取决于给定的数据就可以学会看、学会听、学会感觉，就好像可能有一个算法只依赖于给定的数据，就能学会相应的处理输入。</p><figure><img src="'+S+'" alt="5.22 科学实验1" width="560" tabindex="0" loading="lazy"><figcaption>5.22 科学实验1</figcaption></figure><ol><li><p>有一些系统可以将摄像头安装在某人的前额上，并将其映射到某人舌头上，通过网格中的电压模式，通过将灰度图像映射到舌头，可以帮助学习用舌头看东西。</p></li><li><p>使用回声定位或声纳进行试验，像海豚和蝙蝠使用声纳来观察，如果训练人类发出弹舌的声音，并聆听声音如何从周围环境中反弹，人类有时可以学习某种程度的回声定位。</p></li><li><p>触觉腰带，我在斯坦福大学的实验室做过类似的东西，如果在腰间安装一圈蜂鸣器，并使用磁罗盘对其进行编程，大概就是蜂鸣器指向北方，总是缓慢振动，不知何故就获得了方向感，有些动物有这样的能力，但人类没有。就感觉像你在漫无目的的走，莫名的就是知道北方在哪里，不会觉得我腰部那一部分在振动，在嗡嗡作响。</p></li><li><p>通过手术将第三只眼睛植入青蛙的身上，大脑就可以通过这样的输入一起学习。<strong>因为青蛙植入眼睛的图片违规，所以 3 和 4 如有需要请自行查找图片。</strong></p></li></ol><p>已经有各种各样的实验，表明人类的大脑具有惊人的适应性、惊人的可塑性，它意味着能够适应令人眼花缭乱的数据类型的输入。问题是，如果同一块脑组织，可以学会看、学会摸、甚至其他东西，我们可以复制这个算法，并在计算机上实现吗？</p><p>直到今天，我仍然认为 AGI 是有史以来最引人入胜的科学和工程问题之一，也许有一天你会选择对它进行研究。然而，我认为避免过度炒作很重要，我不知道大脑是否真的是一个或一小部分算法，即使是，我或者其他人也不知道那算法是什么。但我仍然有这个希望，也许它就是这样，也许我们可以通过大量的努力工作，有一天能发现它的近似值。</p>',67))])}const Ls=g(P,[["render",Bs]]),Us=JSON.parse('{"path":"/blogs/MachineLearning/MachineLearning/05_deep_learning.html","title":"2-1 神经网络初探","lang":"en-US","frontmatter":{"title":"2-1 神经网络初探","date":"2024-02-02T00:00:00.000Z","categories":["机器学习"],"tags":["神经网络","深度学习"]},"headers":[{"level":2,"title":"1.深度学习的起源和发展","slug":"_1-深度学习的起源和发展","link":"#_1-深度学习的起源和发展","children":[{"level":3,"title":"1.1 发展历程","slug":"_1-1-发展历程","link":"#_1-1-发展历程","children":[]},{"level":3,"title":"1.2 神经元和大脑","slug":"_1-2-神经元和大脑","link":"#_1-2-神经元和大脑","children":[]},{"level":3,"title":"1.3 快速兴起的原因","slug":"_1-3-快速兴起的原因","link":"#_1-3-快速兴起的原因","children":[]}]},{"level":2,"title":"2.模拟商品的需求预测","slug":"_2-模拟商品的需求预测","link":"#_2-模拟商品的需求预测","children":[{"level":3,"title":"2.1 简单预测","slug":"_2-1-简单预测","link":"#_2-1-简单预测","children":[]},{"level":3,"title":"2.2 复杂预测","slug":"_2-2-复杂预测","link":"#_2-2-复杂预测","children":[]},{"level":3,"title":"2.3 神经网络术语","slug":"_2-3-神经网络术语","link":"#_2-3-神经网络术语","children":[]}]},{"level":2,"title":"3.向前传播算法（经典架构）","slug":"_3-向前传播算法-经典架构","link":"#_3-向前传播算法-经典架构","children":[{"level":3,"title":"3.1 神经网络层解析","slug":"_3-1-神经网络层解析","link":"#_3-1-神经网络层解析","children":[]},{"level":3,"title":"3.2 多层神经网络解析","slug":"_3-2-多层神经网络解析","link":"#_3-2-多层神经网络解析","children":[]},{"level":3,"title":"3.3 手写数字识别","slug":"_3-3-手写数字识别","link":"#_3-3-手写数字识别","children":[]}]},{"level":2,"title":"4.图像感知案例","slug":"_4-图像感知案例","link":"#_4-图像感知案例","children":[{"level":3,"title":"4.1 特征构建 Plus","slug":"_4-1-特征构建-plus","link":"#_4-1-特征构建-plus","children":[]},{"level":3,"title":"4.2 人脸识别","slug":"_4-2-人脸识别","link":"#_4-2-人脸识别","children":[]},{"level":3,"title":"4.3 观察隐藏层","slug":"_4-3-观察隐藏层","link":"#_4-3-观察隐藏层","children":[]},{"level":3,"title":"4.4 识别汽车","slug":"_4-4-识别汽车","link":"#_4-4-识别汽车","children":[]}]},{"level":2,"title":"5.吴恩达教授的小故事","slug":"_5-吴恩达教授的小故事","link":"#_5-吴恩达教授的小故事","children":[{"level":3,"title":"5.1 ANI 与 AGI","slug":"_5-1-ani-与-agi","link":"#_5-1-ani-与-agi","children":[]},{"level":3,"title":"5.2 AGI 发展的困难途径","slug":"_5-2-agi-发展的困难途径","link":"#_5-2-agi-发展的困难途径","children":[]},{"level":3,"title":"5.3 AGI 突破的可能路径","slug":"_5-3-agi-突破的可能路径","link":"#_5-3-agi-突破的可能路径","children":[]}]}],"git":{"createdTime":1757688790000,"updatedTime":1758386370000,"contributors":[{"name":"zjk","email":"1213860588@qq.com","commits":1}]},"filePathRelative":"blogs/MachineLearning/MachineLearning/05_deep_learning.md"}');export{Ls as comp,Us as data};
