import{_ as o,c as l,a as e,b as s,d as a,e as p,r as i,o as c}from"./app-BLf5ix8B.js";const r={},u={href:"https://pytorch.ac.cn/docs/stable/torch.html#creation-ops",target:"_blank",rel:"noopener noreferrer"},k={href:"https://pytorch.ac.cn/tutorials/beginner/basics/tensorqs_tutorial.html",target:"_blank",rel:"noopener noreferrer"},d={href:"https://pytorch.ac.cn/docs/stable/torch.html#indexing-slicing-joining-mutating-ops",target:"_blank",rel:"noopener noreferrer"},m={href:"https://blog.csdn.net/Ethan_Rich/article/details/134799695",target:"_blank",rel:"noopener noreferrer"},v={href:"https://pytorch.ac.cn/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors",target:"_blank",rel:"noopener noreferrer"},b={href:"https://pytorch.ac.cn/docs/stable/torch.html#math-operations",target:"_blank",rel:"noopener noreferrer"},h={href:"https://blog.csdn.net/qq_40728667/article/details/134013899",target:"_blank",rel:"noopener noreferrer"},g={href:"https://blog.csdn.net/weixin_47748259/article/details/135611161",target:"_blank",rel:"noopener noreferrer"},y={href:"https://pytorch.ac.cn/tutorials/beginner/basics/data_tutorial.html",target:"_blank",rel:"noopener noreferrer"},_={href:"https://blog.csdn.net/weixin_41936775/article/details/117160981",target:"_blank",rel:"noopener noreferrer"},f={href:"https://pytorch.ac.cn/tutorials/beginner/basics/transforms_tutorial.html",target:"_blank",rel:"noopener noreferrer"},x={href:"https://blog.csdn.net/AI_dataloads/article/details/133144350",target:"_blank",rel:"noopener noreferrer"},P={href:"https://pytorch.ac.cn/tutorials/beginner/basics/buildmodel_tutorial.html",target:"_blank",rel:"noopener noreferrer"},q={href:"https://aosai.github.io/blog-pages/zh/intelligence/MachineLearning/02_linear_regression.html#_3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-gradient-descent",target:"_blank",rel:"noopener noreferrer"},T={href:"https://blog.csdn.net/qq_35812205/article/details/120814447",target:"_blank",rel:"noopener noreferrer"},w={href:"https://pytorch.ac.cn/tutorials/beginner/basics/autogradqs_tutorial.html",target:"_blank",rel:"noopener noreferrer"},z={href:"https://blog.csdn.net/weixin_46649052/article/details/119718582",target:"_blank",rel:"noopener noreferrer"},A={href:"https://pytorch.ac.cn/tutorials/beginner/basics/optimization_tutorial.html",target:"_blank",rel:"noopener noreferrer"},j={href:"https://blog.csdn.net/m0_52987303/article/details/136509035",target:"_blank",rel:"noopener noreferrer"},N={href:"https://blog.csdn.net/qq_39698985/article/details/141823143",target:"_blank",rel:"noopener noreferrer"},D={href:"https://pytorch.ac.cn/tutorials/beginner/basics/saveloadrun_tutorial.html",target:"_blank",rel:"noopener noreferrer"};function I(U,n){const t=i("ExternalLinkIcon");return c(),l("div",null,[n[24]||(n[24]=e(`<h2 id="_1-tensor-张量" tabindex="-1"><a class="header-anchor" href="#_1-tensor-张量"><span>1. Tensor（张量）</span></a></h2><p>张量是一种特殊的数据结构，不管是 PyTorch 还是 Tensorflow 都是使用张量进行运算，因为它可以在 GPU 或其它硬件加速器上使用，并且针对自动微分进行了优化。</p><p>它有点类似 Numpy 的 ndarray，并且很多张量 API 的使用方式都是相同的，如果你熟悉 Numpy，你会发现 torch 使用起来也会非常顺手，而且它两是可以相互转化的。</p><h3 id="_1-1-初始化" tabindex="-1"><a class="header-anchor" href="#_1-1-初始化"><span>1.1 初始化</span></a></h3><p>PyTorch 创建张量的 API 很多，这里只写一些比较常见的方式，想要了解的更详细，请看相关链接。首先，导入 torch 和 numpy：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">import</span> torch</span>
<span class="line"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><strong>直接从 Python 数据初始化</strong>，数据类型 torch 会自动推断。</li><li><strong>从 Numpy 数组初始化</strong>，Numpy 的 ndarray 可以和 PyTorch 的 Tensor 相互转化。</li><li><strong>从另一个张量初始化</strong>，除非显示覆盖，否则新张量将保留参数张量的属性（形状、数据类型）。</li></ol><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token comment"># 初始化一个名叫 data 的 python 列表，并转化为张量形式</span></span>
<span class="line">data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span></span>
<span class="line">x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 将 data 转化为 ndarray，再将 ndarray 转化为 tensor</span></span>
<span class="line">np_array <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">)</span></span>
<span class="line">x_np <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_array<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 以 x_data 的形式，创建一个全为 1 的张量</span></span>
<span class="line">x_ones <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 以 x_data 的形式，创建一个由随机数组成的张量</span></span>
<span class="line">x_rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Random Tensor: \\n </span><span class="token interpolation"><span class="token punctuation">{</span>x_rand<span class="token punctuation">}</span></span><span class="token string"> \\n&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="4"><li><strong>使用随机值或常量值等</strong>，shape 是一个张量维度的元组，也就是上面所说的张量的形状属性。</li></ol><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token comment"># 定义一个二维的，2行3列 的张量形状</span></span>
<span class="line">shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 以这个张量形状，别分创建由 随机数、全为1、全为0 组成的张量</span></span>
<span class="line">rand_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>shape<span class="token punctuation">)</span></span>
<span class="line">ones_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span></span>
<span class="line">zeros_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Random Tensor: \\n </span><span class="token interpolation"><span class="token punctuation">{</span>rand_tensor<span class="token punctuation">}</span></span><span class="token string"> \\n&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>参考文献&amp;相关链接：</strong></p>`,11)),s("ol",null,[s("li",null,[s("a",u,[n[0]||(n[0]=a("《PyTorch 中文文档 - 张量创建相关 API》",-1)),p(t)])]),s("li",null,[s("a",k,[n[1]||(n[1]=a("《PyTorch 中文文档 - 张量简介》",-1)),p(t)])])]),n[25]||(n[25]=e(`<h3 id="_1-2-维度及属性" tabindex="-1"><a class="header-anchor" href="#_1-2-维度及属性"><span>1.2 维度及属性</span></a></h3><ul><li>0 维：scalar（数值、标量）</li><li>1 维：vector（向量、一维向量）</li><li>2 维：matrix（矩阵、二维向量）</li><li>n 维：n-dimensional tensor（n 维向量）</li></ul><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">x0 <span class="token operator">=</span> tensor<span class="token punctuation">(</span><span class="token number">42.</span><span class="token punctuation">)</span></span>
<span class="line">x1 <span class="token operator">=</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.1</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token number">1.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">x2 <span class="token operator">=</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.1</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token number">1.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.1</span><span class="token punctuation">,</span> <span class="token number">2.2</span><span class="token punctuation">,</span> <span class="token number">2.3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>张量的属性有很多，只列举一部分。但是支持初始化时传参的，仅有前三个：</p><ol><li>init：仅在 init 初始化构造器中使用，支持传入 initializer 的子类</li><li>形状（shape）：是一个由数字组成的元组（tuple）</li><li>数据类型（dtype）：double、float、long、boolean 等</li></ol><ul><li>转置（T）：线性代数里有讲</li><li>单个元素大小（itemsize）：整数，代表每一个元素占用的字节数</li><li>总的字节数量（nbytes）：整数，代表 Tensor 占用的总字节数</li><li>维度数量（ndim）：整数，代表 Tensor 的秩，=len(tensor.shape)</li><li>元素个数（size）：整数，表示一共有多少个元素</li><li>每一维度步长（strides）：元组（tuple），每一个维度所需要的字节数</li><li>储存张量的设备（device）：cpu、gpu</li></ul><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">x1<span class="token punctuation">.</span>ndim   <span class="token comment"># 1</span></span>
<span class="line">x1<span class="token punctuation">.</span>shape  <span class="token comment"># (1, )</span></span>
<span class="line">x2<span class="token punctuation">.</span>T      <span class="token comment"># [[1.1, 2.1], [1.2, 2.2], [1.3, 2.3]]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-3-张量上的操作" tabindex="-1"><a class="header-anchor" href="#_1-3-张量上的操作"><span>1.3 张量上的操作</span></a></h3><p>张量上的操作超过了 100 种，包括算数、线性代数、矩阵操作、采样等等。这些操作都可以运行在 GPU 上，通常比 CPU 上的速度更快。</p><p>但是默认情况下张量是在 CPU 上创建的，我们需要通过 .to 的方法将张量显式的转移到 GPU 上去。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token comment"># 如果 GPU 可用</span></span>
<span class="line"><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_availabel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token comment"># 将数据发送给 GPU，并重新赋值</span></span>
<span class="line">  tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 也可以通过 device() 指定设备</span></span>
<span class="line">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 如果 GPU 可用选择 GPU，否则选择 CPU</span></span>
<span class="line">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_availabel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span></span>
<span class="line">tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>索引和切片操作</strong>，与 Numpy、Python 并无不同：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;First row: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;First column: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token format-spec">, 0]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Last column: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line">tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>张量连接操作</strong>，可以使用 torch.cat 和 torch.stack 对张量进行连接：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span></span>
<span class="line">t1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>t1<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>单元素张量（数值）</strong>，比如你做了求和操作，可以使用 item() 函数将其从张量数据转化为 Python 数据：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span></span>
<span class="line">agg <span class="token operator">=</span> tensor<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">agg_item <span class="token operator">=</span> agg<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>参考文献&amp;相关链接：</strong></p>`,18)),s("ol",null,[s("li",null,[s("a",d,[n[2]||(n[2]=a("《PyTorch 中文文档 - 张量操作及运算 API》",-1)),p(t)])]),s("li",null,[s("a",m,[n[3]||(n[3]=a("《Pytorch 指定设备》",-1)),p(t)])]),s("li",null,[s("a",v,[n[4]||(n[4]=a("《PyTorch 中文文档 - 张量上的操作》",-1)),p(t)])])]),n[26]||(n[26]=e(`<h3 id="_1-4-数学运算" tabindex="-1"><a class="header-anchor" href="#_1-4-数学运算"><span>1.4 数学运算</span></a></h3><p>Tensor 里简单的运算，比如加减乘除、取余（%）、整除（//），可以使用 Python 中的运算符，也可以使用封装好的运算函数。</p><p><strong>逐点运算</strong>：这些基本运算都是对数值的运算，放在 1 维以上的维度中，就是相同位置元素之间的运算，所以两个张量的形状必须一样。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span> <span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">z1 <span class="token operator">=</span> x <span class="token operator">+</span> y</span>
<span class="line">z2 <span class="token operator">=</span> x <span class="token operator">-</span> y</span>
<span class="line">z3 <span class="token operator">=</span> x <span class="token operator">*</span> y</span>
<span class="line">z4 <span class="token operator">=</span> x <span class="token operator">/</span> y</span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> y<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> z1<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> z2<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> z3<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> z4<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">m1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></span>
<span class="line">m2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></span>
<span class="line">m3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></span>
<span class="line">m4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>m1<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> m2<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> m3<span class="token punctuation">,</span> <span class="token string">&quot;\\n&quot;</span><span class="token punctuation">,</span> m4<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>矩阵运算</strong>：在线性代数中，最常用的就是矩阵（向量）的乘法、转置、求逆等操作：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span><span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   <span class="token comment"># 向量默认都是竖向量</span></span>
<span class="line">b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span><span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>   <span class="token comment"># 通过 view 变换成横向量</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 二维矩阵中的矩阵乘法有这 3 种形式</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>a @ b<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 假如参与运算的是一个多维张量，那么只有torch.matmul()可以使用</span></span>
<span class="line"><span class="token comment"># 并且在多维张量中，参与矩阵运算的只有后两个维度，前面的维度就像是索引一样</span></span>
<span class="line">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span></span>
<span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>参考文献&amp;相关链接：</strong></p>`,7)),s("ol",null,[s("li",null,[s("a",b,[n[5]||(n[5]=a("《PyTorch 中文文档 - 数学运算 API》",-1)),p(t)])]),s("li",null,[s("a",h,[n[6]||(n[6]=a("《PyTorch 中的常见运算》",-1)),p(t)])])]),n[27]||(n[27]=e(`<h3 id="_1-5-与-numpy-的桥梁" tabindex="-1"><a class="header-anchor" href="#_1-5-与-numpy-的桥梁"><span>1.5 与 Numpy 的桥梁</span></a></h3><p><strong>torch 张量变换为 Numpy 数组：</strong></p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span></span>
<span class="line">n <span class="token operator">=</span> t<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;t: </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">}</span></span><span class="token string">\\nn: </span><span class="token interpolation"><span class="token punctuation">{</span>n<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 此时如果操作张量 t，numpy 数组同样也会变化</span></span>
<span class="line">t<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;t: </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">}</span></span><span class="token string">\\nn: </span><span class="token interpolation"><span class="token punctuation">{</span>n<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Numpy 数组变换为 torch 张量：</strong></p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line">n <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span></span>
<span class="line">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>n<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;n: </span><span class="token interpolation"><span class="token punctuation">{</span>n<span class="token punctuation">}</span></span><span class="token string">\\nt: </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 此时如果操作 numpy 数组，张量 t 同样也会变化</span></span>
<span class="line">np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> out<span class="token operator">=</span>n<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;n: </span><span class="token interpolation"><span class="token punctuation">{</span>n<span class="token punctuation">}</span></span><span class="token string">\\nt: </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_2-pytorch-常用封装" tabindex="-1"><a class="header-anchor" href="#_2-pytorch-常用封装"><span>2. PyTorch 常用封装</span></a></h2><p>这里列举了一些，做模型训练时经常会用到的函数，或者说 API。仅仅只是介绍一下，有个印象，具体的用法和实例我会在每一个部分都添加几个比较易懂的博文。</p><h3 id="_2-1-数据加载器-数据集" tabindex="-1"><a class="header-anchor" href="#_2-1-数据加载器-数据集"><span>2.1 数据加载器（数据集）</span></a></h3><p>处理数据样本的代码可能会变得混乱且难以维护，理想的情况下，我们希望数据集代码与模型训练代码分离开来，让其可以方便的模块化、以及提高可读性。</p><p>所以 PyTorch 给我们提供了两个处理数据集的 API：</p><ul><li><p>torch.utils.data.Dataset：用于处理单个训练样本，读取数据特征、size、标签等，并且包括数据转换等；</p></li><li><p>torch.utils.data.DataLoader：DataLoader 在 Dataset 周围重载一个可迭代对象，以便轻松访问样本。</p></li></ul><p><strong>参考文献&amp;相关链接：</strong></p>`,12)),s("ol",null,[s("li",null,[s("a",g,[n[7]||(n[7]=a("Dataset 与 DataLoader 使用、构建自定义数据集",-1)),p(t)])]),s("li",null,[s("a",y,[n[8]||(n[8]=a("PyTorch 中文文档 - 数据集 & 数据加载器",-1)),p(t)])])]),n[28]||(n[28]=s("h3",{id:"_2-2-数据变换",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#_2-2-数据变换"},[s("span",null,"2.2 数据变换")])],-1)),n[29]||(n[29]=s("p",null,"一般情况下，预加载的数据集或自己构造的数据集并不能直接用于训练机器学习算法，为了将其转换为训练模型所需的最终形式，我们可以使用 torchvision.transforms 对数据进行处理，以使其适合训练。",-1)),n[30]||(n[30]=s("p",null,"从包名我们就可以看出来，这是一个专门为了计算机视觉（图像）处理而写的 API，不做 CV 的人可以跳过。",-1)),n[31]||(n[31]=s("p",null,[s("strong",null,"参考文献&相关链接：")],-1)),s("ol",null,[s("li",null,[s("a",_,[n[9]||(n[9]=a("Pytorch(三)：数据变换 Transforms",-1)),p(t)])]),s("li",null,[s("a",f,[n[10]||(n[10]=a("PyTorch 中文文档 - 变换",-1)),p(t)])])]),n[32]||(n[32]=s("h3",{id:"_2-3-神经网络模型构建",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#_2-3-神经网络模型构建"},[s("span",null,"2.3 神经网络模型构建")])],-1)),n[33]||(n[33]=s("p",null,"怎么说呢，就是一些简单的机器学习的问题，比如线性回归、逻辑回归等，都可以使用神经网络的形式去进行解决，并且实现层面也比较简单，所以推荐直接从神经网络开始上手。如果有原理什么不懂的，可以查看我的《机器学习》的博文，或者百度。",-1)),n[34]||(n[34]=s("p",null,"PyTorch 里面，neural network 直接被简写成 nn，非常的简洁。 torch.nn 命名空间提供了构建您自己的神经网络所需的所有构建块。PyTorch 中的每个模块都是 nn.Module 的子类。神经网络本身就是一个模块，它由其他模块（层）组成。这种嵌套结构允许轻松构建和管理复杂的架构。",-1)),n[35]||(n[35]=s("p",null,[s("strong",null,"参考文献&相关链接：")],-1)),s("ol",null,[s("li",null,[s("a",x,[n[11]||(n[11]=a("神经网络模型（最细的手写字识别案例）",-1)),p(t)])]),s("li",null,[s("a",P,[n[12]||(n[12]=a("PyTorch 中文文档 - 构建神经网络",-1)),p(t)])])]),n[36]||(n[36]=s("h3",{id:"_2-4-自动微分-求导",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#_2-4-自动微分-求导"},[s("span",null,"2.4 自动微分（求导）")])],-1)),n[37]||(n[37]=s("p",null,[a("第一次跟着 b 站博主写鸢尾花分类问题的代码时，我还不太理解为什么会有"),s("strong",null,"反向传播"),a("这个操作，而且每次都要做，每一个迭代还都要清零一次。")],-1)),n[38]||(n[38]=s("p",null,[a("我们知道神经网络，是从输入层，到隐藏层（1 to n），再到输出层，这是一步一步向前走的，叫做"),s("strong",null,"向前传播"),a("，在继承 nn.Module 的类时，必须要重写的一个类函数就是它，def forward(self)这样子。")],-1)),s("p",null,[n[14]||(n[14]=s("strong",null,"反向传播",-1)),n[15]||(n[15]=a("顾名思义，就是从后往前走，主要是",-1)),s("a",q,[n[13]||(n[13]=a("梯度下降",-1)),p(t)]),n[16]||(n[16]=a("时需要用到。为了计算这些梯度，PyTorch 有一个内置的微分引擎，称为 torch.autograd。它支持自动计算任何计算图的梯度。只需要在张量初始化的时候，加入一个属性：requires_grad=True 即可。",-1))]),n[39]||(n[39]=s("p",null,[s("strong",null,"参考文献&相关链接：")],-1)),s("ol",null,[s("li",null,[s("a",T,[n[17]||(n[17]=a("【PyTorch 基础教程 4】反向传播与计算图（学不会来打我啊）",-1)),p(t)])]),s("li",null,[s("a",w,[n[18]||(n[18]=a("PyTorch 中文文档 - 使用 torch.autograd 进行自动微分",-1)),p(t)])])]),n[40]||(n[40]=e('<h3 id="_2-5-参数优化" tabindex="-1"><a class="header-anchor" href="#_2-5-参数优化"><span>2.5 参数优化</span></a></h3><p>这里的参数优化主要是指<strong>超参数</strong>，它是可调整的参数，允许您控制模型优化过程。不同的超参数值会影响模型训练和收敛速度。</p><p>经过吴恩达教授的机器学习课程，我们也积累的很多类型的超参数，比如：</p><ul><li>线性回归里的：学习率 alpha（α）</li><li>逻辑回归里的：正则化参数 lambda（λ）</li><li>神经网络里的：迭代训练次数、批量大小</li><li>......</li></ul><p>PyTorch 内置了很多类型的参数优化器，比如 SGD 优化器，ADAM 优化器，RMSProp 优化器等等，它们适用于不同类型的模型和数据。</p><p><strong>参考文献&amp;相关链接：</strong></p>',6)),s("ol",null,[s("li",null,[s("a",z,[n[19]||(n[19]=a("PyTorch 学习—13.优化器 optimizer 的概念及常用优化器",-1)),p(t)])]),s("li",null,[s("a",A,[n[20]||(n[20]=a("PyTorch 中文文档 - 优化模型参数",-1)),p(t)])])]),n[41]||(n[41]=s("h3",{id:"_2-6-模型的保存和加载",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#_2-6-模型的保存和加载"},[s("span",null,"2.6 模型的保存和加载")])],-1)),n[42]||(n[42]=s("p",null,"保存模型就是为了再次使用，不管我们是在这个保存的数据基础上进一步的训练优化，还是我们去做迁移学习、共享参数，我们都得先把这个训练好的模型记录下来。",-1)),n[43]||(n[43]=s("p",null,[s("strong",null,"参考文献&相关链接：")],-1)),s("ol",null,[s("li",null,[s("a",j,[n[21]||(n[21]=a("PyTorch 中的模型保存：一键保存、两种选择/保存整个模型和保存模型参数",-1)),p(t)])]),s("li",null,[s("a",N,[n[22]||(n[22]=a("pytorch 模型保存及加载参数恢复训练的例子",-1)),p(t)])]),s("li",null,[s("a",D,[n[23]||(n[23]=a("PyTorch 中文文档 - 保存和加载模型",-1)),p(t)])])])])}const E=o(r,[["render",I]]),G=JSON.parse('{"path":"/blogs/DeepLearning/PyTorch/01_base.html","title":"PyTorch基础知识","lang":"en-US","frontmatter":{"title":"PyTorch基础知识","date":"2024-09-13T00:00:00.000Z","category":["PyTorch"],"tag":["PyTorch基础"]},"headers":[{"level":2,"title":"1. Tensor（张量）","slug":"_1-tensor-张量","link":"#_1-tensor-张量","children":[{"level":3,"title":"1.1 初始化","slug":"_1-1-初始化","link":"#_1-1-初始化","children":[]},{"level":3,"title":"1.2 维度及属性","slug":"_1-2-维度及属性","link":"#_1-2-维度及属性","children":[]},{"level":3,"title":"1.3 张量上的操作","slug":"_1-3-张量上的操作","link":"#_1-3-张量上的操作","children":[]},{"level":3,"title":"1.4 数学运算","slug":"_1-4-数学运算","link":"#_1-4-数学运算","children":[]},{"level":3,"title":"1.5 与 Numpy 的桥梁","slug":"_1-5-与-numpy-的桥梁","link":"#_1-5-与-numpy-的桥梁","children":[]}]},{"level":2,"title":"2. PyTorch 常用封装","slug":"_2-pytorch-常用封装","link":"#_2-pytorch-常用封装","children":[{"level":3,"title":"2.1 数据加载器（数据集）","slug":"_2-1-数据加载器-数据集","link":"#_2-1-数据加载器-数据集","children":[]},{"level":3,"title":"2.2 数据变换","slug":"_2-2-数据变换","link":"#_2-2-数据变换","children":[]},{"level":3,"title":"2.3 神经网络模型构建","slug":"_2-3-神经网络模型构建","link":"#_2-3-神经网络模型构建","children":[]},{"level":3,"title":"2.4 自动微分（求导）","slug":"_2-4-自动微分-求导","link":"#_2-4-自动微分-求导","children":[]},{"level":3,"title":"2.5 参数优化","slug":"_2-5-参数优化","link":"#_2-5-参数优化","children":[]},{"level":3,"title":"2.6 模型的保存和加载","slug":"_2-6-模型的保存和加载","link":"#_2-6-模型的保存和加载","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"filePathRelative":"blogs/DeepLearning/PyTorch/01_base.md"}');export{E as comp,G as data};
