<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-rc.19">
    <script>
      (function() {
        const userMode = localStorage.getItem('vuepress-reco-color-scheme') || 'auto';
        const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;

        if (userMode === 'dark' || (userMode === 'auto' && systemDarkMode)) {
          document.documentElement.classList.toggle('dark', true);
        }
      })();
    </script>
    <title>3-3 强化学习 | AoSaiX</title><meta name="description" content="Blog for study, life and hobby.">
    <link rel="preload" href="/blog-vp-reco/assets/style-7OIMBXxo.css" as="style"><link rel="stylesheet" href="/blog-vp-reco/assets/style-7OIMBXxo.css">
    <link rel="modulepreload" href="/blog-vp-reco/assets/app-DxZOFQRu.js"><link rel="modulepreload" href="/blog-vp-reco/assets/11_reinforcement_learning.html-CqCyqdH8.js">
    <link rel="prefetch" href="/blog-vp-reco/assets/timeline.html-DV4nD8e1.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/posts.html-BaVjNe7k.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/friendship-link.html-CRrTf5B1.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/1.html-D-QNFjVK.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/2.html-BjMT-0D7.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/3.html-CnlaoYDJ.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/4.html-Drf9tSS0.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-DjuGlcDd.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-C_w-cJKn.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_compress_photo.html-OEK2dxI9.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_file_packaging.html-B1s2VC6B.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/03_pyqt5_recording.html-BbQjFGxI.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/04_spider_for_ticket.html-Bwt4fgzB.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-0MOBr-52.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-CJp6kFpi.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/vuepress-recojiaocheng.html-D079Y1_I.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_fargoing.html-zhe3HgJc.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_sichuan.html-CbMuTaeN.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-7nKvCSKg.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01.html-gZw_mfhv.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-BdP503wy.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_linear_regression.html-Fa2j8JZK.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/03_linear_regression.html-DdwJg2Cw.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/04_classification.html-Dif1Rk-d.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/05_deep_learning.html-BwtB-Q7O.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/06_tensorflow.html-B5mLOwin.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/07_model_evaluation.html-CesNOzi2.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/08_decision_tree.html-Cws7ukN2.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/09_unsupervised_learning.html-DGx3vRb8.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/10_recommendation_system.html-B0_5GPFh.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-B9rDrHLv.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01.html-HlZeavpy.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-4pfj951s.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01.html-BTJlDnwo.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-T-ULThrO.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_base.html-C_0AwBdW.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_project1.html-CwHsKLkW.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_project2.html-CSLIT8ZV.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/03_project1.html-iszj5sK7.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-ZY_o3z8y.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_recommend_model.html-gKzQKWA9.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/index.html-BvH2Fw-n.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_zongshu.html-BanvFB-u.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_jichucaozuo.html-BKGyA5nm.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/03_zaoshengyulvbo.html-PQ11jXO8.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/04_xinhaoyutuxiang.html-LY1xZjnW.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/05_xingtaixueyutezheng.html-ClxO10eE.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/06_yasuoyubianma.html-DpmnPOaV.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_zongshu.html-CooFZBHu.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_xxx.html-CP751rFB.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/03_xxx.html-C-1KEXJt.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/01_zongshu.html-DdMXCOX8.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/02_xxx.html-BSHkkFqv.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/03_xxx.html-BcxocDfL.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/404.html-DImc71XU.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/Valine.min-_LyT3bfY.js" as="script"><link rel="prefetch" href="/blog-vp-reco/assets/giscus-aTimukGI-DWEKOTfS.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container show-series show-catalog"><header class="navbar-container not-open"><div class="navbar-inner"><div class="site-brand nav-item"><img class="logo" src="/blog-vp-reco/logo1.svg" alt="AoSaiX"><a href="/blog-vp-reco/" class="site-name can-hide">AoSaiX</a></div><div class="nav-item navbar-links-wrapper" style=""><div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form></div><nav class="navbar-links"><!--[--><div class="navbar-links__item"><div class="dropdown-link"><button class="dropdown-link__title" type="button" aria-label="旧博客迁移(未整理)"><span class="xicon-container left title"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->旧博客迁移(未整理)<!--]--></span></span><span class="arrow down"></span></button><button class="dropdown-link--mobile__title" type="button" aria-label="旧博客迁移(未整理)"><span class="title"><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->旧博客迁移(未整理)<!--]--></span></span></span><span class="right arrow"></span></button><ul style="display:none;" class="dropdown-link__container"><!--[--><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/desktop_app/README.md" class="link" aria-label="桌面软件开发"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->桌面软件开发<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/readme" class="link" aria-label="机器学习"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->机器学习<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/intelligence/Matplotlib/readme" class="link" aria-label="Matplotlib"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Matplotlib<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/intelligence/Numpy/readme" class="link" aria-label="Numpy"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Numpy<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/intelligence/PyTorch/readme" class="link" aria-label="PyTorch"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->PyTorch<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/intelligence/recommendationSystem/readme" class="link" aria-label="推荐系统"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->推荐系统<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/dobetter/mahjong/README" class="link" aria-label="雀神之路"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->雀神之路<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/blogs/dobetter/musictheroy/README" class="link" aria-label="音乐之旅"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->音乐之旅<!--]--></span></span><!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-links__item"><div class="dropdown-link"><button class="dropdown-link__title" type="button" aria-label="计算机视觉"><span class="xicon-container left title"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->计算机视觉<!--]--></span></span><span class="arrow down"></span></button><button class="dropdown-link--mobile__title" type="button" aria-label="计算机视觉"><span class="title"><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->计算机视觉<!--]--></span></span></span><span class="right arrow"></span></button><ul style="display:none;" class="dropdown-link__container"><!--[--><li class="dropdown-link__item"><a href="/blog-vp-reco/series/ComputerVision/ImageProcessing/01_综述" class="link" aria-label="图像处理"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->图像处理<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/series/ComputerVision/ImageGeneration/01_综述" class="link" aria-label="生图模型"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->生图模型<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blog-vp-reco/series/ComputerVision/TargetDetection/01_综述" class="link" aria-label="目标检测"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->目标检测<!--]--></span></span><!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-links__item"><div class="dropdown-link"><button class="dropdown-link__title" type="button" aria-label="学思随录"><span class="xicon-container left title"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->学思随录<!--]--></span></span><span class="arrow down"></span></button><button class="dropdown-link--mobile__title" type="button" aria-label="学思随录"><span class="title"><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->学思随录<!--]--></span></span></span><span class="right arrow"></span></button><ul style="display:none;" class="dropdown-link__container"><!--[--><li class="dropdown-link__item"><a href="/blog-vp-reco/docs/2025/vuepress-reco教程" class="link" aria-label="2025年"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2025年<!--]--></span></span><!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-links__item"><div class="dropdown-link"><button class="dropdown-link__title" type="button" aria-label="星河驿站"><span class="xicon-container left title"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->星河驿站<!--]--></span></span><span class="arrow down"></span></button><button class="dropdown-link--mobile__title" type="button" aria-label="星河驿站"><span class="title"><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->星河驿站<!--]--></span></span></span><span class="right arrow"></span></button><ul style="display:none;" class="dropdown-link__container"><!--[--><li class="dropdown-link__item"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#" class="router-link-active router-link-exact-active link" aria-label="烟火人间"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->烟火人间<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#" class="router-link-active router-link-exact-active link" aria-label="逐趣成章"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->逐趣成章<!--]--></span></span><!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--><!----><span class="xicon-container btn-toggle-dark-mode btn--dark-mode navbar-links__item"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" style="width:20px;height:20px;font-size:20px;color:;"><path d="M15 2h2v3h-2z" fill="currentColor"></path><path d="M27 15h3v2h-3z" fill="currentColor"></path><path d="M15 27h2v3h-2z" fill="currentColor"></path><path d="M2 15h3v2H2z" fill="currentColor"></path><path d="M5.45 6.884l1.414-1.415l2.121 2.122l-1.414 1.414z" fill="currentColor"></path><path d="M23 7.58l2.121-2.12l1.414 1.414l-2.121 2.121z" fill="currentColor"></path><path d="M23.002 24.416l1.415-1.414l2.12 2.122l-1.413 1.414z" fill="currentColor"></path><path d="M5.47 25.13L7.59 23L9 24.42l-2.12 2.12l-1.41-1.41z" fill="currentColor"></path><path d="M16 8a8 8 0 1 0 8 8a8 8 0 0 0-8-8zm0 14a6 6 0 0 1 0-12z" fill="currentColor"></path></svg></span><ul class="social-links navbar-links__item"><!--[--><!--]--></ul></nav><span class="xicon-container btn-toggle-menus"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" style="width:20px;height:20px;font-size:20px;color:;"><circle cx="16" cy="8" r="2" fill="currentColor"></circle><circle cx="16" cy="16" r="2" fill="currentColor"></circle><circle cx="16" cy="24" r="2" fill="currentColor"></circle></svg></span></div></div></header><header class="sub-navbar-container not-open"><span class="nav-item"><div class="toggle-series-button" aria-expanded="false" role="button" tabindex="0"><span></span><span></span><span></span></div> Series </span></header><!----><!----><div class="theme-main" style=""><aside class="series-container"><!--[--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->桌面软件开发<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/desktop_app/" class="link series-item series-item" aria-label="桌面级应用开发相关"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->桌面级应用开发相关<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/desktop_app/01_compress_photo.html" class="link series-item series-item" aria-label="Python桌面压缩图片小工具"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Python桌面压缩图片小工具<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/desktop_app/02_file_packaging.html" class="link series-item series-item" aria-label="Python文件打包exe方法"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Python文件打包exe方法<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/desktop_app/03_pyqt5_recording.html" class="link series-item series-item" aria-label="PyQt5使用及踩坑小记"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->PyQt5使用及踩坑小记<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/desktop_app/04_spider_for_ticket.html" class="link series-item series-item" aria-label="Python网络爬虫-爬取12306余票信息"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Python网络爬虫-爬取12306余票信息<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1 active"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->机器学习<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/" class="link series-item series-item" aria-label="0-0 机器学习目录与概述"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->0-0 机器学习目录与概述<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/02_linear_regression.html" class="link series-item series-item" aria-label="1-1 一元线性回归"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1-1 一元线性回归<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/03_linear_regression.html" class="link series-item series-item" aria-label="1-2 多元线性回归"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1-2 多元线性回归<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/04_classification.html" class="link series-item series-item" aria-label="1-3 分类问题"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1-3 分类问题<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/05_deep_learning.html" class="link series-item series-item" aria-label="2-1 神经网络初探"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2-1 神经网络初探<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/06_tensorflow.html" class="link series-item series-item" aria-label="2-2 神经网络进阶"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2-2 神经网络进阶<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/07_model_evaluation.html" class="link series-item series-item" aria-label="2-3 模型评估"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2-3 模型评估<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/08_decision_tree.html" class="link series-item series-item" aria-label="2-4 决策树模型"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2-4 决策树模型<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/09_unsupervised_learning.html" class="link series-item series-item" aria-label="3-1 无监督学习"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3-1 无监督学习<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/MachineLearning/10_recommendation_system.html" class="link series-item series-item" aria-label="3-2 推荐系统"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3-2 推荐系统<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html" class="router-link-active router-link-exact-active link router-link-active series-item series-item" aria-label="3-3 强化学习"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3-3 强化学习<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->Matplotlib<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/Matplotlib/" class="link series-item series-item" aria-label="画布结构详解"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->画布结构详解<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/Matplotlib/01.html" class="link series-item series-item" aria-label="常见图及属性"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->常见图及属性<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->Numpy<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/Numpy/" class="link series-item series-item" aria-label="Numpy之拜入宗门"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Numpy之拜入宗门<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/Numpy/01.html" class="link series-item series-item" aria-label="Numpy之熬炼筋骨"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Numpy之熬炼筋骨<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->PyTorch<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/PyTorch/" class="link series-item series-item" aria-label="PyTorch入门手册"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->PyTorch入门手册<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/PyTorch/01_base.html" class="link series-item series-item" aria-label="PyTorch基础知识"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->PyTorch基础知识<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/PyTorch/02_project1.html" class="link series-item series-item" aria-label="实战1-离散分类问题"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->实战1-离散分类问题<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/PyTorch/02_project2.html" class="link series-item series-item" aria-label="实战1-参考答案"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->实战1-参考答案<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/PyTorch/03_project1.html" class="link series-item series-item" aria-label="实战2-线性拟合问题"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->实战2-线性拟合问题<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->推荐系统<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/recommendationSystem/" class="link series-item series-item" aria-label="推荐系统基础 - 01"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->推荐系统基础 - 01<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/intelligence/recommendationSystem/01_recommend_model.html" class="link series-item series-item" aria-label="推荐系统基础 - 02"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->推荐系统基础 - 02<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->雀神之路<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/dobetter/mahjong/" class="link series-item series-item" aria-label="麻将大全"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->麻将大全<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/dobetter/mahjong/01_fargoing.html" class="link series-item series-item" aria-label="麻将通识"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->麻将通识<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/dobetter/mahjong/02_sichuan.html" class="link series-item series-item" aria-label="四川麻将"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->四川麻将<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--[--><section class="series-group series-item"><h5 class="series-heading series-level-1"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:16px;"><!--[-->音乐之旅<!--]--></span></span><span class="arrow down"></span></h5><ul style="display:block;"><li><!--[--><a href="/blog-vp-reco/blogs/dobetter/musictheroy/" class="link series-item series-item" aria-label="音乐理论中的科学"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->音乐理论中的科学<!--]--></span></span><!--[--><!--]--></a><!--]--></li><li><!--[--><a href="/blog-vp-reco/blogs/dobetter/musictheroy/01.html" class="link series-item series-item" aria-label="吉他的乐理基础"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->吉他的乐理基础<!--]--></span></span><!--[--><!--]--></a><!--]--></li></ul></section><!--]--><!--]--></aside><!--[--><main class="page-container"><div class="page-content"><h1 class="page-title">3-3 强化学习</h1><div class="page-info"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M16 4a5 5 0 1 1-5 5a5 5 0 0 1 5-5m0-2a7 7 0 1 0 7 7a7 7 0 0 0-7-7z" fill="currentColor"></path><path d="M26 30h-2v-5a5 5 0 0 0-5-5h-6a5 5 0 0 0-5 5v5H6v-5a7 7 0 0 1 7-7h6a7 7 0 0 1 7 7z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->AOSAI<!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M26 4h-4V2h-2v2h-8V2h-2v2H6c-1.1 0-2 .9-2 2v20c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 22H6V12h20v14zm0-16H6V6h4v2h2V6h8v2h2V6h4v4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2024-08-28<!--]--></span></span><!----><!----><!----></div><div class="theme-reco-md-content"><div><p>强化学习（Reinforcement Learning）</p><h2 id="_1-强化学习概述" tabindex="-1"><a class="header-anchor" href="#_1-强化学习概述"><span>1. 强化学习概述</span></a></h2><p>在机器学习中，强化学习与其说是一类算法，不如说是一种思想，就和 <strong>贪心、动态规划、分治、回溯</strong> 这些经典的解决思路一样。虽然目前尚未在商业领域中得到广泛应用，但它也是机器学习的支柱之一。</p><p>我们还是从一个例子开始，图 11.1 是斯坦福大学自主研发的遥感直升机，重 32 磅。与其他遥感直升机一样，它配备了机载计算机、GPS、加速度计、陀螺仪和磁罗盘，因此它可以随时非常准确的知道自己的位置。</p><div class="layout"><figure><img src="/blog-vp-reco/machinelearning/five/11-01.png" alt="11.1 强化学习-遥感直升机1" width="360" tabindex="0" loading="lazy"><figcaption>11.1 强化学习-遥感直升机1</figcaption></figure><figure><img src="/blog-vp-reco/machinelearning/five/11-02.png" alt="11.2 强化学习-遥感直升机2" width="360" tabindex="0" loading="lazy"><figcaption>11.2 强化学习-遥感直升机2</figcaption></figure></div><p>遥感直升机的正常操作，就像 90 年代用手柄打卡带游戏一样，通过操作两个摇杆，以及不同的功能按钮，保持直升机在空中的平衡和飞行。</p><p>图 11.2 是吴恩达教授驾驶摇杆直升机的图像，仔细观察会发现，这个直升机它在倒着飞，有点像空中杂技。没错，它就是用强化学习做到的。如果你有兴趣看更多的视频，可以 <a href="http://heli.stanford.edu" target="_blank" rel="noopener noreferrer">点击此处<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。</p><p>那么问题来了，如果给你一架摇杆直升机的密钥，让你来编写一个程序去自主驾驶它，你会怎么做？</p><h3 id="_1-1-什么是强化学习" tabindex="-1"><a class="header-anchor" href="#_1-1-什么是强化学习"><span>1.1 什么是强化学习</span></a></h3><p>假设我们给定一个任务：通过直升机的位置来决定如何移动驾驶杆。</p><p>我们将直升机的位置、方向、速度等称为状态 s。目标任务是找到一个函数，将直升机的状态映射到动作 a，即将两个操作杆推多远，以保证直升机在空中飞行时保持平衡不会坠毁。</p><p>这个任务也许能通过监督学习完成。比如我们有大量的状态观察结果，并且有一位专业的人类飞行员告诉我们应该采取的最佳行动是什么。然后你就可以使用监督学习训练神经网络，以直接学习 x（s）到标签 y（动作 a）的映射。</p><p>但事实证明，当直升机在空中移动时，“应该采取什么正确的行动”这个问题是很模糊的。比如向左倾斜时是一点还是很大？或者增加直升机压力是稍微还是很多？得到 x 和理想动作 y 的数据集是非常困难的。</p><p>这就是为什么对于很多控制机器人的任务，监督学习方法效果不佳，从而改为使用强化学习。<strong>强化学习的一个关键输入，叫做奖励（函数），它会告诉算法什么时候做的好，什么时候做的不好。</strong></p><p>对于奖励函数，在吴恩达教授看来就像是训练小狗。如何让小狗表现良好呢？你不能向小狗展示太多东西，相反，你只是让它做自己的事情，如果做的好，就鼓励夸夸，如果做了坏事，就凶它骂它。然后希望它自己学习如何做更多好的事情，做更少坏的事情。</p><p>强化学习算法也是这样，做的好的时候夸你，做的不好的时候骂你。比如直升机飞的好的时候，奖励它多飞 1 秒（+1）；飞的不好的时候，给一个负奖励，少飞 1 秒（-1）；如果坠毁，给一个非常大的负奖励，比如（-1000）。</p><h3 id="_1-2-形式-火星探测器" tabindex="-1"><a class="header-anchor" href="#_1-2-形式-火星探测器"><span>1.2 形式（火星探测器）</span></a></h3><figure><img src="/blog-vp-reco/machinelearning/five/11-03.png" alt="11.3 火星探测器1" width="560" tabindex="0" loading="lazy"><figcaption>11.3 火星探测器1</figcaption></figure><p>在这个简化的例子中，探测器有 6 个可能会移动的位置（状态）。假设探测器一开始在状态 4，它可以去不同的地方用它的传感器（探头、雷达、光谱仪等等）来分析火星上不同地方的岩石，或者拍摄有趣的照片供科学家们观看。</p><p>状态 1 和状态 6 都有非常有趣的地质结构，科学家们希望探测器对其采样，但状态 1 的有趣程度为 100 分，状态 6 的有趣程度为 40 分。其余的状态奖励为 0。</p><p>在每一步的决策中，探测器都可以选择向左走或者向右走。我们可以模拟几种情况来做说明：</p><ul><li>一直往左走，奖励为 [0, 0, 0, 100]</li><li>一直往右走，奖励为 [0, 0, 40]</li><li>先往右走一次，再往左走，奖励为 [0, 0, 0, 0, 0, 100]</li></ul><p>第三种情况很明显不太好，但是它也有可能会发生。</p><p>总而言之，每一个阶段，探测机器人都会处于某种状态（称为 s），它可以选择一个动作（称为 a），并且它还有一些从状态中获得的奖励（称为 R(s)），以及因为动作而产生的新的状态（称为 s&#39;）。</p><p>强化学习的核心要素就是这四件事：<strong>状态、动作、奖励、下一个状态</strong>，记录为：（s, a, R(s), s&#39;）。比如探测器从状态 4 往左走一次：（4, left, 0, 3）。</p><p>对于这个应用程序，假设它进入状态 1 或 6 时，这一天就结束了。这种情况在强化学习中被称为<strong>终端状态</strong>，意味着一旦到达终端状态之一后，获得奖励后就结束后续的动作。</p><h3 id="_1-3-回报-火星探测器" tabindex="-1"><a class="header-anchor" href="#_1-3-回报-火星探测器"><span>1.3 回报（火星探测器）</span></a></h3><p>回报这个概念，是强化学习中如何实施动作，做决策的关键。</p><p>做个有意思的类比：假设你站在分叉路口，往左走 5 分钟可以捡到一张 5 美元的钞票，往右走 30 分钟可以捡到一张 10 美元的钞票，你会往哪里走？</p><p>虽然 10 美元看起来比 5 美元好多了，但是如果要你花 30 分钟去拿那张 10 美元，也许你会觉得没有 5 美元来的更方便。</p><p>所以，在这个例子中，回报的概念抓住了你“更快获得奖励”可能比“需要更长时间才能获得奖励”更有吸引力。</p><p><strong>回报被定义为这些奖励的总和，但其中需要一个叫做“折扣因子（Gamma）”的东西加权。</strong> 还是火星探测器的图例：</p><figure><img src="/blog-vp-reco/machinelearning/five/11-03.png" alt="11.3 火星探测器1" width="560" tabindex="0" loading="lazy"><figcaption>11.3 火星探测器1</figcaption></figure><p>假设折扣因子为 0.9，我们从状态 4 一直向左移动到状态 1，它的回报是：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>n</mi><mo>=</mo><mn>0</mn><mo>+</mo><mo stretchy="false">(</mo><mn>0.9</mn><mo stretchy="false">)</mo><mo>⋅</mo><mn>0</mn><mo>+</mo><mo stretchy="false">(</mo><mn>0.9</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>⋅</mo><mn>0</mn><mo>+</mo><mo stretchy="false">(</mo><mn>0.9</mn><msup><mo stretchy="false">)</mo><mn>3</mn></msup><mo>⋅</mo><mn>100</mn><mo>=</mo><mn>72.9</mn></mrow><annotation encoding="application/x-tex">Return=0+(0.9)\cdot{0}+(0.9)^{2}\cdot{0}+(0.9)^{3}\cdot{100}=72.9 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0.9</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord">0</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0.9</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord">0</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0.9</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord"><span class="mord">100</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">72.9</span></span></span></span></span></p><p>我们可以看到，折扣因子 Gamma 是指数型增长，越到后面越小。吴恩达教授的解释很有趣，他说：Gamma 的作用是让强化学习有点不耐烦。这样它就会往奖励越大的、越靠近的状态上靠。</p><p>用符号归纳一下回报的函数，它可以写为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>n</mi><mo>=</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mn>3</mn></msub><mo>+</mo><mo>⋯</mo></mrow><annotation encoding="application/x-tex">Return=R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0585em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.313em;"></span><span class="minner">⋯</span></span></span></span></span></p><p>在许多强化学习算法中，折扣因子的选择是非常接近 1 的数字，比如 0.9，0.99，甚至 0.999。但是为了能从另一个角度，更好的理解回报的原理，这里 Gamma 选择了 0.5。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-04.png" alt="11.4 火星探测器2" width="560" tabindex="0" loading="lazy"><figcaption>11.4 火星探测器2</figcaption></figure><p>图中的三行数据，每一行里红色的数字表示回报，黄色的箭头表示在该状态下往哪边走，黑色的数字代表奖励。三种不同的移动模式所带来的回报，差别是很明显的。</p><h3 id="_1-4-决策-火星探测器" tabindex="-1"><a class="header-anchor" href="#_1-4-决策-火星探测器"><span>1.4 决策（火星探测器）</span></a></h3><p>如同前几个小节所讲，强化学习中可以采取多种不同的方式，去决定下一个动作该做什么，比如在火星探测器中：</p><ol><li>总是追求更接近的奖励。靠近左边就往左边走，靠近右边就往右边走。</li><li>总是追求更大的奖励。状态 1 奖励最大，所以总是往左走。</li><li>总是追求更小的奖励。总是往右走，虽然看起来不是一个好主意，但也是一种选择。</li><li>往更大的奖励走，除非距离较小的奖励仅一步之遥。状态 234 往左走，状态 5 往右走。</li></ol><figure><img src="/blog-vp-reco/machinelearning/five/11-05.png" alt="11.5 火星探测器3" width="560" tabindex="0" loading="lazy"><figcaption>11.5 火星探测器3</figcaption></figure><p>在强化学习中，我们的目标是提出一个称为 策略 Pi 的函数，将任何状态 s 作为输入并将其映射到它希望我们采取的某个动作 a。比如策略 Pi 选择了第 4 种方式，那么探测机器人就会按照第 4 种方式去移动。</p><p>强化学习的目标是找到一个策略 Pi，告诉你在不同的状态下，采取什么行动可以获得最大化的回报。</p><h3 id="_1-5-回顾总结" tabindex="-1"><a class="header-anchor" href="#_1-5-回顾总结"><span>1.5 回顾总结</span></a></h3><p>我们用 6 种状态的火星探测器示例初步讲解了强化学习的形式，让我们快速回顾一下关键概念，并了解如何将这组概念应用于其他的程序。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-06.png" alt="11.6 回顾小结" width="560" tabindex="0" loading="lazy"><figcaption>11.6 回顾小结</figcaption></figure><p>火星探测器和遥感直升机已经在前面说过了，第三个是国际象棋。假设你想使用强化学习来学习下棋：</p><ul><li><strong>状态</strong>：棋盘上所有棋子的位置（简化版）。</li><li><strong>动作</strong>：游戏中合法的移动。</li><li><strong>奖励</strong>：常见方式为，赢了奖励+1，输了负奖励-1，零奖励可能与游戏有关。</li><li><strong>折扣因子</strong>：国际象棋通常 Gamma 为接近 1 的数字，比如 0.99。</li><li><strong>回报</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mn>3</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}} ...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mord">...</span></span></span></span></li><li><strong>决策</strong>：目标棋子被赋予了一个棋盘位置，使用策略 Pi 选择一个好的动作。</li></ul><p>这种强化学习应用程序的形式实际上有一个名字，叫做：<strong>马尔可夫决策过程（Markov Decision Process，MDP）</strong>。它是指：未来仅取决于当前的状态，而不是取决于到达当前状态之前可能发生的任何事情。</p><h2 id="_2-状态-动作价值函数" tabindex="-1"><a class="header-anchor" href="#_2-状态-动作价值函数"><span>2. 状态-动作价值函数</span></a></h2><p>状态-动作价值函数（State-action value function）的目的是为了寻找当前状态下，回报最大的动作。也就是让回报最大化，它的形式可以写为循环：</p><p>Q(s, a) = Return if you</p><ul><li>start in state s.</li><li>take action a (just once).</li><li>then behave optimally after that.</li></ul><figure><img src="/blog-vp-reco/machinelearning/five/11-04.png" alt="11.4 火星探测器2" width="560" tabindex="0" loading="lazy"><figcaption>11.4 火星探测器2</figcaption></figure><p>这是 1.3 小节中讲回报的图，第一行是火星探测车全向左走，第二行是全向右走。我们将这两行的回报结合起来，对比大小来看，第三行其实就是最大回报的动作决策。</p><p>另外，因为这个函数总是被写作 Q 函数，或者 Q*函数（optimal Q function），所以如果你在很多文献里看到了它们，不要惊讶，它们就表示状态-动作价值函数。</p><h3 id="_2-1-代码示例" tabindex="-1"><a class="header-anchor" href="#_2-1-代码示例"><span>2.1 代码示例</span></a></h3><p>这是吴恩达教授简化的代码（<a href="/machinelearning/five/utils.py" target="_blank" rel="noopener noreferrer">utils.py 文件可以从此处打开<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>），他希望我们通过自己修改一些参数，比如更改两边的奖励数值、更改折扣因子的大小等等，看看自动策略会如何根据这些不同的值而变化。</p><p>::: tabs</p><p>@tab 简化版代码</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</span>
<span class="line"><span class="token keyword">from</span> utils <span class="token keyword">import</span> <span class="token operator">*</span></span>
<span class="line"></span>
<span class="line">num_states <span class="token operator">=</span> <span class="token number">6</span></span>
<span class="line">num_actions <span class="token operator">=</span> <span class="token number">2</span></span>
<span class="line"></span>
<span class="line">terminal_left_reward <span class="token operator">=</span> <span class="token number">100</span></span>
<span class="line">terminal_right_reward <span class="token operator">=</span> <span class="token number">40</span></span>
<span class="line">each_step_reward <span class="token operator">=</span> <span class="token number">0</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Discount factor</span></span>
<span class="line">gamma <span class="token operator">=</span> <span class="token number">0.5</span></span>
<span class="line"><span class="token comment">#probability of going in the wrong direction</span></span>
<span class="line">misstep_prob <span class="token operator">=</span> <span class="token number">0</span></span>
<span class="line"></span>
<span class="line">generate_visualization<span class="token punctuation">(</span>terminal_left_reward<span class="token punctuation">,</span> terminal_right_reward<span class="token punctuation">,</span> each_step_reward<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> misstep_prob<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>@tab 运行结果</p><figure><img src="/blog-vp-reco/machinelearning/five/11-07.png" alt="11.7 状态-动作价值函数" width="560" tabindex="0" loading="lazy"><figcaption>11.7 状态-动作价值函数</figcaption></figure><p>:::</p><p>有的同学可能看了运行结果会问，为什么状态 2 和状态 3 向右走的回报，不是 2.5 和 5，反而是 12.5 和 6.25 呢。因为这两个状态向右走的策略是：先往右走一次，然后一直往左走。</p><p>Q 函数的目的是找寻回报最大化的动作，在状态 2 中，12.5 明显比 2.5 大，所以迂回的走法比一直向右走看起来更好，状态 3 同理。</p><p>此外，misstep_prob 这个参数详情，请看 2.3 小节，随机环境。</p><h3 id="_2-2-贝尔曼方程-bellman-equation" tabindex="-1"><a class="header-anchor" href="#_2-2-贝尔曼方程-bellman-equation"><span>2.2 贝尔曼方程（Bellman Equation）</span></a></h3><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mo>⋅</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">Q(s, a)=R(s)+\gamma\cdot{\max{Q(s&#39;, a&#39;)}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0519em;vertical-align:-0.25em;"></span><span class="mord"><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></span></p><p>贝尔曼方程简单的来说，就是告诉我们，当前状态下该动作的回报，是由（1）当前状态的奖励，也称作即时奖励；（2）折扣因子 × 下一个状态的回报最大的动作；两个部分组成。它其实可以看作是对回报的拆分：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>R</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mn>3</mn></msub><mo>+</mo><mo>⋯</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mrow><mo stretchy="false">[</mo><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mn>3</mn></msub><mo>+</mo><mo>⋯</mo><mtext> </mtext><mo stretchy="false">]</mo></mrow></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mo>⋅</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align} Return &amp;= R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots \\ &amp;= R_{1}+\gamma{[R_{2}+\gamma{R_{3}}+\cdots]} \\ &amp;= R(s)+\gamma\cdot{\max{Q(s&#39;, a&#39;)}} \end{align} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.5241em;vertical-align:-2.0121em;"></span><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5121em;"><span style="top:-4.6479em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">n</span></span></span><span style="top:-3.1479em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-1.6479em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0121em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5121em;"><span style="top:-4.6479em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="minner">⋯</span></span></span><span style="top:-3.1479em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mclose">]</span></span></span></span><span style="top:-1.6479em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0121em;"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5121em;"><span style="top:-4.5121em;"><span class="pstrut" style="height:2.8641em;"></span><span class="eqn-num"></span></span><span style="top:-3.0121em;"><span class="pstrut" style="height:2.8641em;"></span><span class="eqn-num"></span></span><span style="top:-1.5121em;"><span class="pstrut" style="height:2.8641em;"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0121em;"><span></span></span></span></span></span></span></span></span></p><figure><img src="/blog-vp-reco/machinelearning/five/11-08.png" alt="11.8 贝尔曼方程" width="560" tabindex="0" loading="lazy"><figcaption>11.8 贝尔曼方程</figcaption></figure><p>图 11.8 中举了两个例子，从状态 2 往右走，计算出来 12.5 就是它的最大回报；从状态 4 往左走，12.5 是它的最大回报。</p><h3 id="_2-3-随机环境" tabindex="-1"><a class="header-anchor" href="#_2-3-随机环境"><span>2.3 随机环境</span></a></h3><p>在实践中，由于刮风、偏离航线、车轮打滑等等原因，许多机器人没有办法完全按照你的要求去做，所以结果并不会总是可靠。</p><p>比如，在火星探测器向左行驶的途中，可能会遇见岩石滑坡，或者地面真的很滑，导致它滑向了错误的方向。</p><p>我们可以模拟它出现错误的概率，比如 90%的概率会正常运行，10%的概率会遭遇意外情况。比如：</p><ul><li>从状态 3 向左走，没有出错：[0, 0, 100]</li><li>从状态 3 向左走，在状态 2 时打滑了一次，回到了状态 3：[0, 0, 0, 0, 100]</li></ul><p>我们会发现，加入概率之后啊，回报就不是一个准确的公式，而是无数个公式的集合，我们需要取它的平均值，或者说是期望，来重新测定每个状态，往左或往右走的回报。</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>E</mi><mi>x</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi><mi mathvariant="normal">_</mi><mi>R</mi><mi>e</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>A</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mo stretchy="false">(</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mn>3</mn></msub><mo>+</mo><mo>⋯</mo><mtext> </mtext><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mn>3</mn></msub><mo>+</mo><mo>⋯</mo><mtext> </mtext><mo stretchy="false">]</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align} Expected\_Return &amp;= Average(R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots) \\ &amp;= E[R_{1}+\gamma{R_{2}}+\gamma^{2}{R_{3}}+\cdots] \end{align} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.0482em;vertical-align:-1.2741em;"></span><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7741em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mord mathnormal">ec</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">n</span></span></span><span style="top:-2.3859em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2741em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7741em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mclose">)</span></span></span><span style="top:-2.3859em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mclose">]</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2741em;"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7741em;"><span style="top:-3.7741em;"><span class="pstrut" style="height:2.8641em;"></span><span class="eqn-num"></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:2.8641em;"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2741em;"><span></span></span></span></span></span></span></span></span></p><p>可以修改 2.1 小节中代码参数 misstep_prob，做对比观察。同理，对于贝尔曼方程，我们也需要改写为期望的形式。</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mo>⋅</mo><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>max</mi><mo>⁡</mo><mrow><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">Q(s, a) = R(s)+\gamma\cdot{E[\max{Q(s&#39;, a&#39;)}]} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0519em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mclose">]</span></span></span></span></span></span></p><h2 id="_3-连续的状态空间" tabindex="-1"><a class="header-anchor" href="#_3-连续的状态空间"><span>3. 连续的状态空间</span></a></h2><p>我们使用的简化版火星探测器，是一组离散的状态，它意味着探测器只可能处于 6 个位置中的一个。</p><p>但事实上，它可以处于大量连续位置中的任何一个。比如，一条横向的 6 公里长的直线，我们以米作为单位，向左或向右走时，[0, 6000]m 以内的任何数字都是有效的。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-09.png" alt="11.9 连续的状态空间" width="560" tabindex="0" loading="lazy"><figcaption>11.9 连续的状态空间</figcaption></figure><p>我们再举一个例子，比如正在行驶的卡车（玩具车），它需要考虑的就不只是一个状态了，比如，前后方向的位置 x，左右方向的位置 y，卡车行驶的方向 θ，以及前后方向的速度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6679em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">˙</span></span></span></span></span></span></span></span></span></span>，左右方向的速度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">˙</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span>，行驶方向/角度变化的速度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>o</mi><mi>t</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">dot{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span>。</p><p>所以卡车的状态将包含由这 6 个符号组成的向量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>θ</mi><mo separator="true">,</mo><mover accent="true"><mi>x</mi><mo>˙</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>˙</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>θ</mi><mo>˙</mo></mover><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">s=[x, y, \theta, \dot{x}, \dot{y}, \dot{\theta}]^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1813em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">˙</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0556em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>，并且这些符号中的任何一个，都可以采用有效范围内的任何值。比如方向/角度的范围为 0 到 360°。</p><h3 id="_3-1-登月器" tabindex="-1"><a class="header-anchor" href="#_3-1-登月器"><span>3.1 登月器</span></a></h3><figure><img src="/blog-vp-reco/machinelearning/five/11-10.png" alt="11.10 登月器" width="560" tabindex="0" loading="lazy"><figcaption>11.10 登月器</figcaption></figure><p>这是一个很有意思的模拟月球着陆的程序，每个时间点你可以有四种操作：</p><ol><li>什么都不做，让惯性和重力将着陆器拉向月球表面。</li><li>启动左侧推进器，将着陆器推向右边移动。</li><li>启动右侧推进器，将着陆器推向左边移动。</li><li>启动底部的主推进器，减缓下降的速度。</li></ol><p>你的任务就是随着时间的推移，不断地选择行动，让着陆器安全的降落在两个黄旗中间的位置。那么它的状态向量都包含些什么呢？</p><ul><li>x 和 y 表示在水平方向和垂直方向的位置</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>˙</mo></mover><mtext>和</mtext><mover accent="true"><mi>y</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot{x}和\dot{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mord cjk_fallback">和</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">˙</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span> 表示在横轴和纵轴上的移动速度</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> 表示着陆器的角度，也就是机身的倾斜程度</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>θ</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9313em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0556em;"><span class="mord">˙</span></span></span></span></span></span></span></span></span></span> 表示倾斜的速度，或者说角速度</li><li>l 和 r 是两个布尔类型的变量，对应：左腿是否着地、右腿是否着地</li></ul><p>它的奖励函数也很有趣，之前的火星探测器，只有两个状态存在奖励，而这次的着陆器有 7 种奖励，并且包括负奖励：</p><figure><img src="/blog-vp-reco/machinelearning/five/11-11.png" alt="11.11 登月器的奖励函数" width="560" tabindex="0" loading="lazy"><figcaption>11.11 登月器的奖励函数</figcaption></figure><ol><li>假设两根黄旗中间有一个垫子，着陆器正在设法降落在上面，我们根据降落途中的飞行情况，给 100~140 的奖励；</li><li>并且有一个额外的奖励，离垫子中心越近，奖励越高，离垫子中心越远，奖励越低；</li><li>如果坠毁，奖励-100；</li><li>软着陆成功，奖励+100；</li><li>每条腿落地，都会奖励+10；</li><li>假设我们鼓励它节省燃料，所以每次启动主引擎（主推进器）时，奖励-0.3；</li><li>每次触发左侧或右侧推进器时，奖励-0.03。</li></ol><p>这是一个中等复杂程度的奖励函数，设计者对真正你想要的行为进行了一些思考，并将其编入奖励函数中，激励更多你想要的行为。</p><p>当你自己构建一个强化学习应用程序时，通常你会花些心思来准确的指定你想要什么，你不想要什么，以及将其编入奖励函数。</p><p>总结一下，着陆器程序的任务是：在给定状态向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mover accent="true"><mi>x</mi><mo>˙</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>˙</mo></mover><mo separator="true">,</mo><mi>θ</mi><mo separator="true">,</mo><mover accent="true"><mi>θ</mi><mo>˙</mo></mover><mo separator="true">,</mo><mi>l</mi><mo separator="true">,</mo><mi>r</mi><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">s=[x, y, \dot{x}, \dot{y}, \theta, \dot{\theta}, l, r]^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1813em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">˙</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0556em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span> 的情况下，让决策 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> 选择一个最佳的动作 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a=\pi(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span>，使得回报最大化，折扣因子选定为 Gamma=0.985。</p><h3 id="_3-2-强化学习中的神经网络" tabindex="-1"><a class="header-anchor" href="#_3-2-强化学习中的神经网络"><span>3.2 强化学习中的神经网络</span></a></h3><p>用强化学习来解决登月器或者其它问题的一个关键思想是，我们要训练一个神经网络来计算或近似 state，action 的状态动作价值函数 Q。这反过来又会让我们选择一个好的行动。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-12.png" alt="11.12 强化学习中的神经网络" width="560" tabindex="0" loading="lazy"><figcaption>11.12 强化学习中的神经网络</figcaption></figure><p>神经网络的输入包含了所有的状态和动作，状态向量已经写过了：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mover accent="true"><mi>x</mi><mo>˙</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>˙</mo></mover><mo separator="true">,</mo><mi>θ</mi><mo separator="true">,</mo><mover accent="true"><mi>θ</mi><mo>˙</mo></mover><mo separator="true">,</mo><mi>l</mi><mo separator="true">,</mo><mi>r</mi><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">s=[x, y, \dot{x}, \dot{y}, \theta, \dot{\theta}, l, r]^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1813em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">˙</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0556em;"><span class="mord">˙</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>；4 个动作我们可以通过 one-hot 进行编码：</p><ul><li>nothing，什么都不做，[1, 0, 0, 0]</li><li>left，启动左侧推进器，[0, 1, 0, 0]</li><li>main，启动主推进器，[0, 0, 1, 0]</li><li>right，启动右侧推进器，[0, 0, 0, 1]</li></ul><p>所以这个由 12 个数字组成的列表或者说向量（8 个状态，4 个动作），我们将其写作神经网络的输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2077em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em;" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span></span></span></span></span></span></span>。</p><p>需要注意的是，我们并不是输入状态就直接让它输出动作（纯粹的监督学习），而是让它输出 Q 函数。神经网络在这里只是强化学习的一个部分。</p><p>对于 Q 函数而言，这种方式的效果很好。那么到这里，问题就变成了：如何训练一个神经网络来输出 Q(s, a) 函数？</p><h3 id="_3-3-构建训练集" tabindex="-1"><a class="header-anchor" href="#_3-3-构建训练集"><span>3.3 构建训练集</span></a></h3><p>首先，该方法将使用贝尔曼方程，来创建包含大量示例（x，y）的训练集，然后使用监督学习中的神经网络做模型训练，将 state-action（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2077em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.471em" height="0.714em" style="width:0.471em;" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span></span></span></span></span></span></span>）映射到目标值 Q(s, a)（y）。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-13.png" alt="11.13 通过贝尔曼方程构建训练集" width="560" tabindex="0" loading="lazy"><figcaption>11.13 通过贝尔曼方程构建训练集</figcaption></figure><p>我们将贝尔曼方程切分为两个部分，等式左边就是输入向量 x，等式右边就是输出 y。</p><p>问题的关键是，我们并不知道什么才是最佳的 Q 函数，能够将回报最大化的下一个状态和动作是什么我们不清楚。</p><p>没关系，我们可以取随机值，来构建一个，比如包含 1 万个数据的训练集。</p><h3 id="_3-4-deep-q-network-dqn" tabindex="-1"><a class="header-anchor" href="#_3-4-deep-q-network-dqn"><span>3.4 Deep Q-Network（DQN）</span></a></h3><p>通过前面几个小节，我们可以概括出神经网络构建的全过程：</p><p><strong>1. 初始化神经网络，随机的猜测 Q(s, a)。</strong></p><p>这就有点像训练线性回归模型时，随机的初始化所有参数，然后使用梯度下降一步一步完善。所以重要的其实是，算法能不能慢慢的改进参数，以获得更好的估计。</p><p><strong>2. 反复执行以下操作：</strong></p><p>2.1 在着陆期间，执行任何的操作，无论是好的还是坏的，你会获得多个这样的元组 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s, a, R(s), s&#39;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><p>2.2 重放缓冲区（Replay Buffer）：储存最新的 1 万个元组数据。</p><p>2.3 训练神经网络：将这 1 万个元组数据，通过贝尔曼方程构建成数据集。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">maxQ(s&#39;, a&#39;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 最开始是随机初始化的，没关系，通过训练慢慢就会变成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mo>⋅</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">Q_{new}(s, a)=R(s)+\gamma\cdot{\max{Q(s&#39;, a&#39;)}} \approx{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span></p><p>2.4 让 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><msub><mi>Q</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q=Q_{new}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><p>我们在反复执行训练神经网路的步骤中，Q(s, a) 是会继承上一次的训练结果的，所以每一次迭代，它都会变成更好的估计。</p><p>所以理论上来讲，只要你迭代的次数够多，训练的时间足够长，这个模拟着陆程序中的 DQN 就会变得足够好。</p><h2 id="_4-算法改进" tabindex="-1"><a class="header-anchor" href="#_4-算法改进"><span>4. 算法改进</span></a></h2><h3 id="_4-1-神经网络架构改进" tabindex="-1"><a class="header-anchor" href="#_4-1-神经网络架构改进"><span>4.1 神经网络架构改进</span></a></h3><figure><img src="/blog-vp-reco/machinelearning/five/11-12.png" alt="11.12 强化学习中的神经网络" width="560" tabindex="0" loading="lazy"><figcaption>11.12 强化学习中的神经网络</figcaption></figure><p>这是原本的神经网络模型，我们需要在每个状态上，分别对四个动作进行推理。也就是一个状态就要运行四次神经网络，或者说每个动作都对应一个神经网络，这是非常低效的。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-14.png" alt="11.14 神经网络架构改进" width="560" tabindex="0" loading="lazy"><figcaption>11.14 神经网络架构改进</figcaption></figure><p>事实证明，在一个神经网络上同时计算并输出四个动作所对应的 Q 函数，效果会比原本的更好。因为通过贝尔曼方程，对比四个动作的回报，一次就可以知道在该状态下，做什么动作是最好的。</p><h3 id="_4-2-ε-贪婪策略-epsilon-greedy-policy" tabindex="-1"><a class="header-anchor" href="#_4-2-ε-贪婪策略-epsilon-greedy-policy"><span>4.2 ε-贪婪策略（Epsilon-greedy policy）</span></a></h3><p>当你处于某些状态时，可能并不想完全随机的采取行动，因为那样通常可能会是一个糟糕的结果。</p><p>我们原本的选项是：（1）选择一个动作 a，尽可能的最大化 Q(s, a)。即使它可能不尽人意，但是算法会尽力的使用我们当前对 Q(s, a)的猜测，并最大化它的收益。</p><p>而现在的选项是：<strong>（1）0.95 的概率，选择一个能最大化 Q(s, a)的动作 a。（2）0.05 的概率，随机选择一个动作 a。</strong></p><p>这么做的原因是，假设在随机初始化 Q(s ,a)的时候，出现了一些不好的情况，比如 Q(s, a)始终都很低，这可能导致该启动主推进器时，一直不启动的类似问题。</p><p>引入一个 0.05 的概率，神经网络可以学会克服自己的先入之见，也许自己之前坚持的才是错的呢？</p><p>这种随机选择动作的想法有时被称为探索步骤（Exploration Step）。还有一个很神奇的地方是，贪婪指的是 0.95 的概率，寻求 Q(s, a)最大化的部分，0.05 的随机探索才是不贪婪的。</p><p>一般来说，ε 的值是从大到小变化的。最初，你可能采取较多的随机行动，因为本身也不知道哪个好，哪个不好；</p><p>随着时间的推移，Q(s, a)迭代很多次后，已经有了很多好的推理，你可能会更倾向于用新的 Q(s, a)来估计和选择行动，所以随机行动就降低了。</p><h3 id="_4-3-超参数的敏感性" tabindex="-1"><a class="header-anchor" href="#_4-3-超参数的敏感性"><span>4.3 超参数的敏感性</span></a></h3><p>在监督学习中，超参数是不那么敏感的，比如，学习率选择的不好，可能也就是会让训练量或者说训练时间翻个两三倍。</p><p>但是强化学习中，超参数是很敏感的，假设你的 ε-贪婪策略中，epsilon 的值选的不够好，训练量可能会翻个 10 倍、100 倍。这非常恐怖。</p><p>吴恩达教授觉得，这可能是因为强化学习算法本身还不够完善导致的。目前，对于一个新项目而言，还需要我们自己试错。</p><h3 id="_4-4-小批量处理-mini-batch" tabindex="-1"><a class="header-anchor" href="#_4-4-小批量处理-mini-batch"><span>4.4 小批量处理（mini-batch）</span></a></h3><p>小批量处理可以加速强化学习和监督学习的训练速度，但是前提是你的训练集真的非常大，我们从监督学习的线性回归说起：</p><figure><img src="/blog-vp-reco/machinelearning/five/11-15.png" alt="11.15 小批量处理1" width="560" tabindex="0" loading="lazy"><figcaption>11.15 小批量处理1</figcaption></figure><p>这是最初的房价预测的线性回归模型，但是假设有 1 亿个训练样本，本来在梯度下降时，更新参数 w 和 b 就要循环迭代很多次，再乘一个 1 亿的样本基数，训练时间会大到难以想想。</p><p><mark>小批量处理要做的事情就是，将其划分为不同的集合，一个集合中 1000 个样本、5000 个样本……<strong>然后在进行成本函数计算时，第一次用集合 1 的样本，第二次用集合 2 的样本，依次类推</strong>。</mark></p><figure><img src="/blog-vp-reco/machinelearning/five/11-16.png" alt="11.16 小批量处理2" width="560" tabindex="0" loading="lazy"><figcaption>11.16 小批量处理2</figcaption></figure><p>一次性处理（Batch learning）的结果是，梯度会直接的向最小的地方前进，但是数据量太大了，计算时间会非常长。</p><p>小批量处理（Mini-Batch learning）的优势在于，计算成本要低得多，所以当数据集很大的时候，它是一个更快的算法。一般会和别的算法一起用，比如 adam。</p><p>缺点也很明显，可能会因为不同集合的样本走歪路，虽然它也会最终趋向于全局最优化，但是它不是很可靠，并且会有噪点。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-17.png" alt="11.17 小批量处理3" width="560" tabindex="0" loading="lazy"><figcaption>11.17 小批量处理3</figcaption></figure><p>回到强化学习中来，我们可以在训练神经网络模型的时候，使用小批量处理，将 1 万个数据，切分成 1000 个数据的不同集合。循环使用不同的集合来训练参数。</p><h3 id="_4-5-软更新" tabindex="-1"><a class="header-anchor" href="#_4-5-软更新"><span>4.5 软更新</span></a></h3><p>软更新可以帮助你的强化学习算法更好的收敛到一个好的解决方案。</p><p>在登月器的最后一步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><msub><mi>Q</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q=Q_{new}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 中，有一个隐患，假如遇到了像小批量处理时，数据集不好的情况下，新的 Q 值可能会更糟糕。</p><figure><img src="/blog-vp-reco/machinelearning/five/11-18.png" alt="11.18 软更新" width="560" tabindex="0" loading="lazy"><figcaption>11.18 软更新</figcaption></figure><p>我们可以给新的参数乘一个 0.01，旧的参数乘一个 0.99，这就是软更新，因为我们只会接受一点点的新值，所以出现错误的可能性大大降低，并且收敛性也会变得更佳。</p></div></div><footer class="page-meta"><!----><div class="meta-item last-updated"><span class="xicon-container left meta-item-label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:20px;height:20px;font-size:20px;color:;"><path d="M26 4h-4V2h-2v2h-8V2h-2v2H6c-1.1 0-2 .9-2 2v20c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 22H6V12h20v14zm0-16H6V6h4v2h2V6h8v2h2V6h4v4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Last Updated 9/12/2025, 10:53:10 PM<!--]--></span></span></div></footer><nav class="page-nav"><p class="hasPrev inner"><span class="page-nav-item prev"> ← 3-2 推荐系统</span><!----></p></nav><!----></div><div class="page-catalog-container"><h5 class="tip">ON THIS PAGE</h5><ul><!--[--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_1-强化学习概述" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="1. 强化学习概述"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1. 强化学习概述<!--]--></span></span><!--[--><!--]--></a></li><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_1-1-什么是强化学习" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="1.1 什么是强化学习"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1.1 什么是强化学习<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_1-2-形式-火星探测器" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="1.2 形式（火星探测器）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1.2 形式（火星探测器）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_1-3-回报-火星探测器" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="1.3 回报（火星探测器）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1.3 回报（火星探测器）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_1-4-决策-火星探测器" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="1.4 决策（火星探测器）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1.4 决策（火星探测器）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_1-5-回顾总结" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="1.5 回顾总结"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->1.5 回顾总结<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_2-状态-动作价值函数" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="2. 状态-动作价值函数"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2. 状态-动作价值函数<!--]--></span></span><!--[--><!--]--></a></li><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_2-1-代码示例" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="2.1 代码示例"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2.1 代码示例<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_2-2-贝尔曼方程-bellman-equation" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="2.2 贝尔曼方程（Bellman Equation）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2.2 贝尔曼方程（Bellman Equation）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_2-3-随机环境" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="2.3 随机环境"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2.3 随机环境<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_3-连续的状态空间" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="3. 连续的状态空间"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3. 连续的状态空间<!--]--></span></span><!--[--><!--]--></a></li><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_3-1-登月器" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="3.1 登月器"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3.1 登月器<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_3-2-强化学习中的神经网络" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="3.2 强化学习中的神经网络"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3.2 强化学习中的神经网络<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_3-3-构建训练集" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="3.3 构建训练集"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3.3 构建训练集<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_3-4-deep-q-network-dqn" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="3.4 Deep Q-Network（DQN）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->3.4 Deep Q-Network（DQN）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_4-算法改进" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="4. 算法改进"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->4. 算法改进<!--]--></span></span><!--[--><!--]--></a></li><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_4-1-神经网络架构改进" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="4.1 神经网络架构改进"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->4.1 神经网络架构改进<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_4-2-ε-贪婪策略-epsilon-greedy-policy" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="4.2 ε-贪婪策略（Epsilon-greedy policy）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->4.2 ε-贪婪策略（Epsilon-greedy policy）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_4-3-超参数的敏感性" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="4.3 超参数的敏感性"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->4.3 超参数的敏感性<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_4-4-小批量处理-mini-batch" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="4.4 小批量处理（mini-batch）"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->4.4 小批量处理（mini-batch）<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blog-vp-reco/blogs/intelligence/MachineLearning/11_reinforcement_learning.html#_4-5-软更新" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="4.5 软更新"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->4.5 软更新<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--><!--]--></ul></div></main><!--]--></div></div><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/blog-vp-reco/assets/app-DxZOFQRu.js" defer></script>
  </body>
</html>
